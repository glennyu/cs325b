{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import datetime\n",
    "import html\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TWEET_PATH = 'deep_learning/data/city_tweets/'\n",
    "NUM_MONTHS = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_date(d):\n",
    "    year = int(d[:4])\n",
    "    month = int(d[5:7])\n",
    "    day = int(d[8:10])\n",
    "    return datetime.date(year, month, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(tweet_file, food_name, city_month_features):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = []\n",
    "    food_cnt = np.zeros(NUM_MONTHS, dtype=np.float32)\n",
    "    month_cnt = np.zeros(NUM_MONTHS, dtype=np.int32)\n",
    "    for i in range(NUM_MONTHS):\n",
    "        sentiment_scores.append(defaultdict(float))\n",
    "        \n",
    "    city_month = tweet_file[:tweet_file.find(\"_tweets\")]\n",
    "    print(\"Processing:\", tweet_file)\n",
    "    with open(TWEET_PATH + tweet_file) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        cnt = 0\n",
    "        for row in reader:\n",
    "            date = get_date(row['postedTime'])\n",
    "            month_index = (date.year - 2014)*12 + (date.month - 1)\n",
    "            tweet = ' '.join([word for word in casual_tokenize(row['tweet']) if '@' not in word and 'http' not in word])\n",
    "            scores = sid.polarity_scores(tweet)\n",
    "            for k, v in scores.items():\n",
    "                sentiment_scores[month_index][k] += v\n",
    "            if food_name in tweet.lower():\n",
    "                food_cnt[month_index] += 1\n",
    "            month_cnt[month_index] += 1\n",
    "            cnt += 1\n",
    "            if (cnt % 200000 == 0): \n",
    "                print(str(cnt),'tweets processed...')\n",
    "    \n",
    "    for i in range(NUM_MONTHS):\n",
    "        if month_cnt[i] != 0:\n",
    "            cur_feat = [food_cnt[i]/month_cnt[i]]\n",
    "            for k in sentiment_scores[i].keys():\n",
    "                cur_feat.append(sentiment_scores[i][k]/month_cnt[i])\n",
    "            city_month_features[city_month + \"_\" + str(i)] = np.array(cur_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_labels(path):\n",
    "    city_month_labels = {}\n",
    "    for filename in os.listdir(path):\n",
    "        city_month = filename[:filename.find(\"_batch\")]\n",
    "        with open(path + filename, \"r\") as batchf:\n",
    "            for batch in batchf:\n",
    "                city_month_labels[city_month] = np.array([int(x) for x in batch.split(',')[0:2]])\n",
    "                break\n",
    "    return city_month_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_file(features, labels, filename):\n",
    "    f = open(filename, \"w\")\n",
    "    for i in range(features.shape[0]):\n",
    "        for j in range(features.shape[1]):\n",
    "            f.write(\"%.10f \" % features[i][j])\n",
    "        f.write('%d %d\\n' % (labels[i][0], labels[i][1]))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate_features_and_labels(city_month_features):\n",
    "    city_month_labels_train = get_labels('deep_learning/data/batches_train/')\n",
    "    city_month_labels_eval = get_labels('deep_learning/data/batches_val/')\n",
    "    train_features, eval_features = [], []\n",
    "    train_labels, eval_labels = [], []\n",
    "    for city_month in city_month_labels_train.keys():\n",
    "        if city_month in city_month_features:\n",
    "            train_features.append(city_month_features[city_month])\n",
    "            train_labels.append(city_month_labels_train[city_month])\n",
    "    for city_month in city_month_labels_eval.keys():\n",
    "        if city_month in city_month_features:\n",
    "            eval_features.append(city_month_features[city_month])\n",
    "            eval_labels.append(city_month_labels_eval[city_month])\n",
    "    train_features = np.array(train_features)\n",
    "    eval_features = np.array(eval_features)\n",
    "    train_labels = np.array(train_labels)\n",
    "    eval_labels = np.array(eval_labels)\n",
    "    print(train_features.shape)\n",
    "    print(eval_features.shape)\n",
    "    print(train_labels.shape)\n",
    "    print(eval_labels.shape)\n",
    "    \n",
    "    write_file(train_features, train_labels, \"train_baseline.txt\")\n",
    "    write_file(eval_features, eval_labels, \"eval_baseline.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\", fontsize=16,\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(title + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_conf_matrix(pred, labels, classes, title):\n",
    "    plt.figure()\n",
    "    conf_matrix = np.zeros((len(classes), len(classes)), dtype=np.float32)\n",
    "    for i in range(labels.shape[0]):\n",
    "        conf_matrix[labels[i]][pred[i]] += 1\n",
    "    plot_confusion_matrix(conf_matrix, \n",
    "                          classes=classes, \n",
    "                          normalize=True, \n",
    "                          title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Processing:', 'Kohima_tweets.csv')\n",
      "('Processing:', 'Cuttack_tweets.csv')\n",
      "('Processing:', 'Lucknow_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('400000', 'tweets processed...')\n",
      "('Processing:', 'Panchkula_tweets.csv')\n",
      "('Processing:', 'Siliguri_tweets.csv')\n",
      "('Processing:', 'Delhi_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('400000', 'tweets processed...')\n",
      "('600000', 'tweets processed...')\n",
      "('800000', 'tweets processed...')\n",
      "('1000000', 'tweets processed...')\n",
      "('1200000', 'tweets processed...')\n",
      "('1400000', 'tweets processed...')\n",
      "('1600000', 'tweets processed...')\n",
      "('1800000', 'tweets processed...')\n",
      "('2000000', 'tweets processed...')\n",
      "('2200000', 'tweets processed...')\n",
      "('2400000', 'tweets processed...')\n",
      "('2600000', 'tweets processed...')\n",
      "('2800000', 'tweets processed...')\n",
      "('3000000', 'tweets processed...')\n",
      "('3200000', 'tweets processed...')\n",
      "('3400000', 'tweets processed...')\n",
      "('3600000', 'tweets processed...')\n",
      "('3800000', 'tweets processed...')\n",
      "('4000000', 'tweets processed...')\n",
      "('4200000', 'tweets processed...')\n",
      "('4400000', 'tweets processed...')\n",
      "('4600000', 'tweets processed...')\n",
      "('4800000', 'tweets processed...')\n",
      "('5000000', 'tweets processed...')\n",
      "('5200000', 'tweets processed...')\n",
      "('5400000', 'tweets processed...')\n",
      "('5600000', 'tweets processed...')\n",
      "('5800000', 'tweets processed...')\n",
      "('6000000', 'tweets processed...')\n",
      "('6200000', 'tweets processed...')\n",
      "('Processing:', 'Ernakulam_tweets.csv')\n",
      "('Processing:', 'Ahmedabad_tweets.csv')\n",
      "('Processing:', 'Bathinda_tweets.csv')\n",
      "('Processing:', 'Sambalpur_tweets.csv')\n",
      "('Processing:', 'Ranchi_tweets.csv')\n",
      "('Processing:', 'Bengaluru_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('400000', 'tweets processed...')\n",
      "('600000', 'tweets processed...')\n",
      "('800000', 'tweets processed...')\n",
      "('1000000', 'tweets processed...')\n",
      "('1200000', 'tweets processed...')\n",
      "('1400000', 'tweets processed...')\n",
      "('1600000', 'tweets processed...')\n",
      "('1800000', 'tweets processed...')\n",
      "('2000000', 'tweets processed...')\n",
      "('2200000', 'tweets processed...')\n",
      "('2400000', 'tweets processed...')\n",
      "('2600000', 'tweets processed...')\n",
      "('2800000', 'tweets processed...')\n",
      "('3000000', 'tweets processed...')\n",
      "('3200000', 'tweets processed...')\n",
      "('3400000', 'tweets processed...')\n",
      "('3600000', 'tweets processed...')\n",
      "('Processing:', 'Shillong_tweets.csv')\n",
      "('Processing:', 'Kanpur_tweets.csv')\n",
      "('Processing:', 'Mumbai_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('400000', 'tweets processed...')\n",
      "('600000', 'tweets processed...')\n",
      "('800000', 'tweets processed...')\n",
      "('1000000', 'tweets processed...')\n",
      "('1200000', 'tweets processed...')\n",
      "('1400000', 'tweets processed...')\n",
      "('1600000', 'tweets processed...')\n",
      "('1800000', 'tweets processed...')\n",
      "('2000000', 'tweets processed...')\n",
      "('2200000', 'tweets processed...')\n",
      "('2400000', 'tweets processed...')\n",
      "('2600000', 'tweets processed...')\n",
      "('2800000', 'tweets processed...')\n",
      "('3000000', 'tweets processed...')\n",
      "('3200000', 'tweets processed...')\n",
      "('3400000', 'tweets processed...')\n",
      "('3600000', 'tweets processed...')\n",
      "('3800000', 'tweets processed...')\n",
      "('4000000', 'tweets processed...')\n",
      "('4200000', 'tweets processed...')\n",
      "('4400000', 'tweets processed...')\n",
      "('4600000', 'tweets processed...')\n",
      "('4800000', 'tweets processed...')\n",
      "('5000000', 'tweets processed...')\n",
      "('5200000', 'tweets processed...')\n",
      "('5400000', 'tweets processed...')\n",
      "('5600000', 'tweets processed...')\n",
      "('5800000', 'tweets processed...')\n",
      "('6000000', 'tweets processed...')\n",
      "('6200000', 'tweets processed...')\n",
      "('6400000', 'tweets processed...')\n",
      "('6600000', 'tweets processed...')\n",
      "('6800000', 'tweets processed...')\n",
      "('7000000', 'tweets processed...')\n",
      "('7200000', 'tweets processed...')\n",
      "('Processing:', 'Hyderabad_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('400000', 'tweets processed...')\n",
      "('600000', 'tweets processed...')\n",
      "('800000', 'tweets processed...')\n",
      "('1000000', 'tweets processed...')\n",
      "('Processing:', 'Chennai_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('Processing:', 'Thiruchirapalli_tweets.csv')\n",
      "('Processing:', 'Jabalpur_tweets.csv')\n",
      "('Processing:', 'Shimla_tweets.csv')\n",
      "('Processing:', 'Agra_tweets.csv')\n",
      "('Processing:', 'Vijaywada_tweets.csv')\n",
      "('Processing:', 'Bhopal_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('Processing:', 'Jammu_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('Processing:', 'Jaipur_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('400000', 'tweets processed...')\n",
      "('Processing:', 'Srinagar_tweets.csv')\n",
      "('Processing:', 'Rourkela_tweets.csv')\n",
      "('Processing:', 'Puducherry_tweets.csv')\n",
      "('Processing:', 'Ludhiana_tweets.csv')\n",
      "('Processing:', 'Nagpur_tweets.csv')\n",
      "('Processing:', 'Karnal_tweets.csv')\n",
      "('Processing:', 'Dehradun_tweets.csv')\n",
      "('Processing:', 'Patna_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('Processing:', 'Dindigul_tweets.csv')\n",
      "('Processing:', 'Rajkot_tweets.csv')\n",
      "('Processing:', 'Bhagalpur_tweets.csv')\n",
      "('Processing:', 'Guwahati_tweets.csv')\n",
      "('Processing:', 'Raipur_tweets.csv')\n",
      "('Processing:', 'Port Blair_tweets.csv')\n",
      "('Processing:', 'Agartala_tweets.csv')\n",
      "('Processing:', 'Trivandrum_tweets.csv')\n",
      "('Processing:', 'Jodhpur_tweets.csv')\n",
      "('Processing:', 'Gwalior_tweets.csv')\n",
      "('Processing:', 'Amritsar_tweets.csv')\n",
      "('Processing:', 'Itanagar_tweets.csv')\n",
      "('Processing:', 'Bhubaneshwar_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('Processing:', 'Kolkata_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('400000', 'tweets processed...')\n",
      "('600000', 'tweets processed...')\n",
      "('800000', 'tweets processed...')\n",
      "('1000000', 'tweets processed...')\n",
      "('1200000', 'tweets processed...')\n",
      "('1400000', 'tweets processed...')\n",
      "('1600000', 'tweets processed...')\n",
      "('1800000', 'tweets processed...')\n",
      "('Processing:', 'Panaji_tweets.csv')\n",
      "('Processing:', 'Gurgaon_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('400000', 'tweets processed...')\n",
      "('600000', 'tweets processed...')\n",
      "('800000', 'tweets processed...')\n",
      "('1000000', 'tweets processed...')\n",
      "('Processing:', 'Hisar_tweets.csv')\n",
      "('Processing:', 'Mandi_tweets.csv')\n",
      "('Processing:', 'Aizwal_tweets.csv')\n",
      "('Processing:', 'Kozhidoke_tweets.csv')\n",
      "('Processing:', 'Indore_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('Processing:', 'Kota_tweets.csv')\n",
      "('Processing:', 'Chandigarh_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('Processing:', 'Dharwad_tweets.csv')\n",
      "('Processing:', 'Varanasi_tweets.csv')\n",
      "('Processing:', 'Dimapur_tweets.csv')\n",
      "(186, 5)\n",
      "(70, 5)\n",
      "(186, 2)\n",
      "(70, 2)\n"
     ]
    }
   ],
   "source": [
    "city_month_features = {}\n",
    "for tweets_file in os.listdir(TWEET_PATH):\n",
    "    get_features(tweets_file, \"onion\", city_month_features)\n",
    "aggregate_features_and_labels(city_month_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('deep_learning/data/tweet_features.txt', \"w\")\n",
    "for city_month, feat in city_month_features.items():\n",
    "    f.write(city_month)\n",
    "    for i in range(feat.shape[0]):\n",
    "        f.write(\"\\t%.10f\" % feat[i])\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 5)\n",
      "(70, 5)\n",
      "(186, 2)\n",
      "(70, 2)\n"
     ]
    }
   ],
   "source": [
    "train_features, eval_features = [], []\n",
    "train_labels, eval_labels = [], []\n",
    "\n",
    "train_f = open(\"train_baseline.txt\", \"r\")\n",
    "eval_f = open(\"eval_baseline.txt\", \"r\")\n",
    "for line in train_f:\n",
    "    train_features.append([float(x) for x in line.split()[:-2]])\n",
    "    train_labels.append([int(x) for x in line.split()[-2:]])\n",
    "for line in eval_f:\n",
    "    eval_features.append([float(x) for x in line.split()[:-2]])\n",
    "    eval_labels.append([int(x) for x in line.split()[-2:]])\n",
    "eval_f.close()\n",
    "\n",
    "train_features = np.array(train_features)\n",
    "eval_features = np.array(eval_features)\n",
    "train_labels = np.array(train_labels)\n",
    "eval_labels = np.array(eval_labels)\n",
    "print(train_features.shape)\n",
    "print(eval_features.shape)\n",
    "print(train_labels.shape)\n",
    "print(eval_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69 55 62]\n",
      "[31 18 21]\n",
      "('Train accuracy:', 0.44086021505376344)\n",
      "('Evaluation accuracy:', 0.5142857142857142)\n",
      "('Coefficients:', array([[-257.8800819 ,   29.50878007,   -2.61738445,  -21.09057279,\n",
      "          14.35929479],\n",
      "       [-502.11986814,  -25.62925131,    3.90576732,   13.21348329,\n",
      "          -9.63812288],\n",
      "       [ 759.99995005,   -3.87952876,   -1.28838287,    7.87708949,\n",
      "          -4.72117192]]))\n"
     ]
    }
   ],
   "source": [
    "clf_price_direction = linear_model.RidgeClassifier(alpha=0.01, normalize=True)\n",
    "clf_price_direction.fit(train_features, train_labels[:,0])\n",
    "train_label_dist = np.zeros(3, dtype=np.int32)\n",
    "for i in range(train_labels.shape[0]):\n",
    "    train_label_dist[train_labels[i][0]] += 1\n",
    "print(train_label_dist)\n",
    "eval_label_dist = np.zeros(3, dtype=np.int32)\n",
    "for i in range(eval_labels.shape[0]):\n",
    "    eval_label_dist[eval_labels[i][0]] += 1\n",
    "print(eval_label_dist)\n",
    "print(\"Train accuracy:\", clf_price_direction.score(train_features, train_labels[:,0]))\n",
    "print(\"Evaluation accuracy:\", clf_price_direction.score(eval_features, eval_labels[:,0]))\n",
    "print(\"Coefficients:\", clf_price_direction.coef_)\n",
    "get_conf_matrix(clf_price_direction.predict(train_features), train_labels[:,0], \n",
    "                ['decrease', 'no change', 'increase'], 'Ridge Classifier Price Direction Train')\n",
    "get_conf_matrix(clf_price_direction.predict(eval_features), eval_labels[:,0], \n",
    "                ['decrease', 'no change', 'increase'], 'Ridge Classifier Price Direction Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104  82]\n",
      "[30 40]\n",
      "('Train accuracy:', 0.6236559139784946)\n",
      "('Evaluation accuracy:', 0.4857142857142857)\n",
      "('Coefficients:', array([[496.88124087,   4.44311155,  -1.81362203, -11.91238662,\n",
      "          9.74698041]]))\n"
     ]
    }
   ],
   "source": [
    "clf_price_spike = linear_model.RidgeClassifier(alpha=0.01, normalize=True)\n",
    "clf_price_spike.fit(train_features, train_labels[:,1])\n",
    "train_label_dist = np.zeros(2, dtype=np.int32)\n",
    "for i in range(train_labels.shape[0]):\n",
    "    train_label_dist[train_labels[i][1]] += 1\n",
    "print(train_label_dist)\n",
    "eval_label_dist = np.zeros(2, dtype=np.int32)\n",
    "for i in range(eval_labels.shape[0]):\n",
    "    eval_label_dist[eval_labels[i][1]] += 1\n",
    "print(eval_label_dist)\n",
    "print(\"Train accuracy:\", clf_price_spike.score(train_features, train_labels[:,1]))\n",
    "print(\"Evaluation accuracy:\", clf_price_spike.score(eval_features, eval_labels[:,1]))\n",
    "print(\"Coefficients:\", clf_price_spike.coef_)\n",
    "get_conf_matrix(clf_price_spike.predict(train_features), train_labels[:,1], \n",
    "                ['no spike', 'spike'], 'Ridge Classifier Price Spike Train')\n",
    "get_conf_matrix(clf_price_spike.predict(eval_features), eval_labels[:,1], \n",
    "                ['no spike', 'spike'], 'Ridge Classifier Price Spike Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
