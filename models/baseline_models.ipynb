{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import datetime\n",
    "import html\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEET_PATH = 'deep_learning/data/city_tweets/'\n",
    "NUM_MONTHS = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(d):\n",
    "    year = int(d[:4])\n",
    "    month = int(d[5:7])\n",
    "    day = int(d[8:10])\n",
    "    return datetime.date(year, month, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(tweet_file, food_name, city_month_features):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = []\n",
    "    food_cnt = np.zeros(NUM_MONTHS, dtype=np.float32)\n",
    "    month_cnt = np.zeros(NUM_MONTHS, dtype=np.int32)\n",
    "    for i in range(NUM_MONTHS):\n",
    "        sentiment_scores.append(defaultdict(float))\n",
    "        \n",
    "    city_month = tweet_file[:tweet_file.find(\"_tweets\")]\n",
    "    print(\"Processing:\", tweet_file)\n",
    "    with open(TWEET_PATH + tweet_file) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        cnt = 0\n",
    "        for row in reader:\n",
    "            date = get_date(row['postedTime'])\n",
    "            month_index = (date.year - 2014)*12 + (date.month - 1)\n",
    "            tweet = ' '.join([word for word in casual_tokenize(row['tweet']) if '@' not in word and 'http' not in word])\n",
    "            scores = sid.polarity_scores(tweet)\n",
    "            for k, v in scores.items():\n",
    "                sentiment_scores[month_index][k] += v\n",
    "            if food_name in tweet.lower():\n",
    "                food_cnt[month_index] += 1\n",
    "            month_cnt[month_index] += 1\n",
    "            cnt += 1\n",
    "            if (cnt % 200000 == 0): \n",
    "                print(str(cnt),'tweets processed...')\n",
    "    \n",
    "    for i in range(NUM_MONTHS):\n",
    "        if month_cnt[i] != 0:\n",
    "            cur_feat = [food_cnt[i]/month_cnt[i]]\n",
    "            for k in sentiment_scores[i].keys():\n",
    "                cur_feat.append(sentiment_scores[i][k]/month_cnt[i])\n",
    "            city_month_features[city_month + \"_\" + str(i)] = np.array(cur_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(path):\n",
    "    city_month_labels = {}\n",
    "    for filename in os.listdir(path):\n",
    "        city_month = filename[:filename.find(\"_batch\")]\n",
    "        with open(path + filename, \"r\") as batchf:\n",
    "            for batch in batchf:\n",
    "                city_month_labels[city_month] = np.array([int(x) for x in batch.split(',')[0:2]])\n",
    "                break\n",
    "    return city_month_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(features, labels, filename):\n",
    "    f = open(filename, \"w\")\n",
    "    for i in range(features.shape[0]):\n",
    "        for j in range(features.shape[1]):\n",
    "            f.write(\"%.10f \" % features[i][j])\n",
    "        f.write('%d %d\\n' % (labels[i][0], labels[i][1]))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_features_and_labels(city_month_features):\n",
    "    city_month_labels_train = get_labels('deep_learning/data/batches_train/')\n",
    "    city_month_labels_eval = get_labels('deep_learning/data/batches_val/')\n",
    "    train_features, eval_features = [], []\n",
    "    train_labels, eval_labels = [], []\n",
    "    for city_month in city_month_labels_train.keys():\n",
    "        if city_month in city_month_features:\n",
    "            train_features.append(city_month_features[city_month])\n",
    "            train_labels.append(city_month_labels_train[city_month])\n",
    "    for city_month in city_month_labels_eval.keys():\n",
    "        if city_month in city_month_features:\n",
    "            eval_features.append(city_month_features[city_month])\n",
    "            eval_labels.append(city_month_labels_eval[city_month])\n",
    "    train_features = np.array(train_features)\n",
    "    eval_features = np.array(eval_features)\n",
    "    train_labels = np.array(train_labels)\n",
    "    eval_labels = np.array(eval_labels)\n",
    "    print(train_features.shape)\n",
    "    print(eval_features.shape)\n",
    "    print(train_labels.shape)\n",
    "    print(eval_labels.shape)\n",
    "    \n",
    "    write_file(train_features, train_labels, \"train_baseline.txt\")\n",
    "    write_file(eval_features, eval_labels, \"eval_baseline.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\", fontsize=16,\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(title + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf_matrix(pred, labels, classes, title):\n",
    "    plt.figure()\n",
    "    conf_matrix = np.zeros((len(classes), len(classes)), dtype=np.float32)\n",
    "    for i in range(labels.shape[0]):\n",
    "        conf_matrix[labels[i]][pred[i]] += 1\n",
    "    plot_confusion_matrix(conf_matrix, \n",
    "                          classes=classes, \n",
    "                          normalize=True, \n",
    "                          title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'deep_learning/data/city_tweets/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a8fc378b2771>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcity_month_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtweets_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTWEET_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"onion\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcity_month_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maggregate_features_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity_month_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'deep_learning/data/city_tweets/'"
     ]
    }
   ],
   "source": [
    "city_month_features = {}\n",
    "for tweets_file in os.listdir(TWEET_PATH):\n",
    "    get_features(tweets_file, \"onion\", city_month_features)\n",
    "aggregate_features_and_labels(city_month_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('deep_learning/data/tweet_features.txt', \"w\")\n",
    "for city_month, feat in city_month_features.items():\n",
    "    f.write(city_month)\n",
    "    for i in range(feat.shape[0]):\n",
    "        f.write(\"\\t%.10f\" % feat[i])\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 5)\n",
      "(70, 5)\n",
      "(186, 2)\n",
      "(70, 2)\n"
     ]
    }
   ],
   "source": [
    "train_features, eval_features = [], []\n",
    "train_labels, eval_labels = [], []\n",
    "\n",
    "train_f = open(\"train_baseline.txt\", \"r\")\n",
    "eval_f = open(\"eval_baseline.txt\", \"r\")\n",
    "for line in train_f:\n",
    "    train_features.append([float(x) for x in line.split()[:-2]])\n",
    "    train_labels.append([int(x) for x in line.split()[-2:]])\n",
    "for line in eval_f:\n",
    "    eval_features.append([float(x) for x in line.split()[:-2]])\n",
    "    eval_labels.append([int(x) for x in line.split()[-2:]])\n",
    "eval_f.close()\n",
    "\n",
    "train_features = np.array(train_features)\n",
    "eval_features = np.array(eval_features)\n",
    "train_labels = np.array(train_labels)\n",
    "eval_labels = np.array(eval_labels)\n",
    "print(train_features.shape)\n",
    "print(eval_features.shape)\n",
    "print(train_labels.shape)\n",
    "print(eval_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69 55 62]\n",
      "[31 18 21]\n",
      "('Train accuracy:', 0.44086021505376344)\n",
      "('Evaluation accuracy:', 0.5142857142857142)\n",
      "('Coefficients:', array([[-257.8800819 ,   29.50878007,   -2.61738445,  -21.09057279,\n",
      "          14.35929479],\n",
      "       [-502.11986814,  -25.62925131,    3.90576732,   13.21348329,\n",
      "          -9.63812288],\n",
      "       [ 759.99995005,   -3.87952876,   -1.28838287,    7.87708949,\n",
      "          -4.72117192]]))\n"
     ]
    }
   ],
   "source": [
    "clf_price_direction = linear_model.RidgeClassifier(alpha=0.01, normalize=True)\n",
    "clf_price_direction.fit(train_features, train_labels[:,0])\n",
    "train_label_dist = np.zeros(3, dtype=np.int32)\n",
    "for i in range(train_labels.shape[0]):\n",
    "    train_label_dist[train_labels[i][0]] += 1\n",
    "print(train_label_dist)\n",
    "eval_label_dist = np.zeros(3, dtype=np.int32)\n",
    "for i in range(eval_labels.shape[0]):\n",
    "    eval_label_dist[eval_labels[i][0]] += 1\n",
    "print(eval_label_dist)\n",
    "print(\"Train accuracy:\", clf_price_direction.score(train_features, train_labels[:,0]))\n",
    "print(\"Evaluation accuracy:\", clf_price_direction.score(eval_features, eval_labels[:,0]))\n",
    "print(\"Coefficients:\", clf_price_direction.coef_)\n",
    "get_conf_matrix(clf_price_direction.predict(train_features), train_labels[:,0], \n",
    "                ['decrease', 'no change', 'increase'], 'Ridge Classifier Price Direction Train')\n",
    "get_conf_matrix(clf_price_direction.predict(eval_features), eval_labels[:,0], \n",
    "                ['decrease', 'no change', 'increase'], 'Ridge Classifier Price Direction Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69 55 62]\n",
      "[31 18 21]\n",
      "('Train accuracy:', 0.5913978494623656)\n",
      "('Evaluation accuracy:', 0.5571428571428572)\n",
      "('Coefficients:', array([0.2891629 , 0.20622108, 0.10566019, 0.20597175, 0.19298408]))\n"
     ]
    }
   ],
   "source": [
    "rf_price_direction = RandomForestClassifier(n_estimators=83, random_state=0, max_depth=2)\n",
    "rf_price_direction.fit(train_features, train_labels[:,0])\n",
    "train_label_dist = np.zeros(3, dtype=np.int32)\n",
    "for i in range(train_labels.shape[0]):\n",
    "    train_label_dist[train_labels[i][0]] += 1\n",
    "print(train_label_dist)\n",
    "eval_label_dist = np.zeros(3, dtype=np.int32)\n",
    "for i in range(eval_labels.shape[0]):\n",
    "    eval_label_dist[eval_labels[i][0]] += 1\n",
    "print(eval_label_dist)\n",
    "print(\"Train accuracy:\", rf_price_direction.score(train_features, train_labels[:,0]))\n",
    "print(\"Evaluation accuracy:\", rf_price_direction.score(eval_features, eval_labels[:,0]))\n",
    "print(\"Coefficients:\", rf_price_direction.feature_importances_)\n",
    "get_conf_matrix(rf_price_direction.predict(train_features), train_labels[:,0], \n",
    "                ['decrease', 'no change', 'increase'], 'Random Forest Price Direction Train')\n",
    "get_conf_matrix(rf_price_direction.predict(eval_features), eval_labels[:,0], \n",
    "                ['decrease', 'no change', 'increase'], 'Random Forest Price Direction Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104  82]\n",
      "[30 40]\n",
      "('Train accuracy:', 0.6236559139784946)\n",
      "('Evaluation accuracy:', 0.5857142857142857)\n",
      "('Coefficients:', array([0.22222222, 0.33333333, 0.44444444, 0.        , 0.        ]))\n"
     ]
    }
   ],
   "source": [
    "rf_price_spike = RandomForestClassifier(n_estimators=9, max_depth=1, random_state=0)\n",
    "rf_price_spike.fit(train_features, train_labels[:,1])\n",
    "train_label_dist = np.zeros(2, dtype=np.int32)\n",
    "for i in range(train_labels.shape[0]):\n",
    "    train_label_dist[train_labels[i][1]] += 1\n",
    "print(train_label_dist)\n",
    "eval_label_dist = np.zeros(2, dtype=np.int32)\n",
    "for i in range(eval_labels.shape[0]):\n",
    "    eval_label_dist[eval_labels[i][1]] += 1\n",
    "print(eval_label_dist)\n",
    "print(\"Train accuracy:\", rf_price_spike.score(train_features, train_labels[:,1]))\n",
    "print(\"Evaluation accuracy:\", rf_price_spike.score(eval_features, eval_labels[:,1]))\n",
    "print(\"Coefficients:\", rf_price_spike.feature_importances_)\n",
    "get_conf_matrix(rf_price_spike.predict(train_features), train_labels[:,1], \n",
    "                ['no spike', 'spike'], 'Random Forest Price Spike Train')\n",
    "get_conf_matrix(rf_price_spike.predict(eval_features), eval_labels[:,1], \n",
    "                ['no spike', 'spike'], 'Random Forest Price Spike Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
