{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.tokenize import casual_tokenize\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_PATH = '../data/'\n",
    "TWEET_PATH = '/mnt/mounted_bucket/'\n",
    "week_words = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings shape: (1193514, 50)\n",
      "Number of keywords: 319\n"
     ]
    }
   ],
   "source": [
    "embedding_keywords = set()\n",
    "word_dict = {}\n",
    "word_list = []\n",
    "word_embeddings = []\n",
    "with open(EMBEDDINGS_PATH + \"glove.twitter.27B.50d.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        word = line.split()[0]\n",
    "        for day in week_words:\n",
    "            if day in word:\n",
    "                embedding_keywords.add(len(word_embeddings))\n",
    "                break\n",
    "        word_dict[word] = len(word_embeddings)\n",
    "        word_list.append(word)\n",
    "        word_embeddings.append([float(x) for x in line.split()[1:]])\n",
    "        while (len(word_embeddings[-1]) < 50):\n",
    "            word_embeddings[-1].append(0.0)\n",
    "        word_embeddings[-1] = word_embeddings[-1][:50]\n",
    "word_embeddings = np.array(word_embeddings)\n",
    "print(\"Word embeddings shape:\", word_embeddings.shape)\n",
    "print(\"Number of keywords:\", len(embedding_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_to_feat_index = {}\n",
    "for i, idx in enumerate(embedding_keywords):\n",
    "    keyword_to_feat_index[idx] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(d):\n",
    "    year = int(d[:4])\n",
    "    month = int(d[5:7])\n",
    "    day = int(d[8:10])\n",
    "    return datetime.date(year, month, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(tweet_file):\n",
    "    date_to_tweets = defaultdict(list)\n",
    "    with open(TWEET_PATH + tweet_file) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        missing_words = set()\n",
    "        cnt = 0\n",
    "        for row in reader:\n",
    "            date = get_date(row['postedTime'])\n",
    "            tweet = ' '.join([word for word in casual_tokenize(row['tweet']) if '@' not in word and 'http' not in word])\n",
    "            tweet = tweet.strip('#').lower()\n",
    "            tweet_embedding = []\n",
    "            for word in tweet.split():\n",
    "                if word in word_dict:\n",
    "                    tweet_embedding.append(word_dict[word])\n",
    "                else:\n",
    "                    tweet_embedding.append(-1)\n",
    "                    missing_words.add(word)\n",
    "            date_to_tweets[date].append(tweet_embedding)\n",
    "            cnt += 1\n",
    "            if (cnt % 200000 == 0): \n",
    "                print(str(cnt),'tweets processed...')\n",
    "        print(\"Number of missing words:\", len(missing_words))\n",
    "    return date_to_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_labels(date_to_tweets):\n",
    "    features, labels = [], []\n",
    "    for date in date_to_tweets:\n",
    "        hist = defaultdict(int)\n",
    "        for tweet in date_to_tweets[date]:\n",
    "            for idx in tweet:\n",
    "                if idx in embedding_keywords:\n",
    "                    hist[idx] += 1\n",
    "        feat = np.zeros(len(embedding_keywords), dtype=np.float32)\n",
    "        for idx, cnt in hist.items():\n",
    "            feat[keyword_to_feat_index[idx]] = cnt\n",
    "        features.append(feat)\n",
    "        labels.append(date.weekday())\n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_features(features, labels):\n",
    "    conf_matrix = np.zeros((7, 7), dtype=np.int32)\n",
    "    day_dict = {}\n",
    "    for i, word in enumerate(week_words):\n",
    "        day_dict[word] = i\n",
    "        day_dict[word + 's'] = i\n",
    "    for i in range(features.shape[0]):\n",
    "        max_idx = np.argmax(features[i])\n",
    "        for key, val in keyword_to_feat_index.items():\n",
    "            if val == max_idx:\n",
    "                if word_list[key] not in day_dict:\n",
    "                    print(\"true:\", week_words[date.weekday()], \"max:\", word_list[key])\n",
    "                else:\n",
    "                    conf_matrix[labels[i]][day_dict[word_list[key]]] += 1\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "400000 tweets processed...\n",
      "500000 tweets processed...\n",
      "600000 tweets processed...\n",
      "700000 tweets processed...\n",
      "800000 tweets processed...\n",
      "900000 tweets processed...\n",
      "1000000 tweets processed...\n",
      "1100000 tweets processed...\n",
      "1200000 tweets processed...\n",
      "1300000 tweets processed...\n",
      "1400000 tweets processed...\n",
      "1500000 tweets processed...\n",
      "1600000 tweets processed...\n",
      "1700000 tweets processed...\n",
      "1800000 tweets processed...\n",
      "1900000 tweets processed...\n",
      "2000000 tweets processed...\n",
      "2100000 tweets processed...\n",
      "2200000 tweets processed...\n",
      "2300000 tweets processed...\n",
      "2400000 tweets processed...\n",
      "2500000 tweets processed...\n",
      "2600000 tweets processed...\n",
      "2700000 tweets processed...\n",
      "2800000 tweets processed...\n",
      "2900000 tweets processed...\n",
      "3000000 tweets processed...\n",
      "3100000 tweets processed...\n",
      "3200000 tweets processed...\n",
      "3300000 tweets processed...\n",
      "3400000 tweets processed...\n",
      "3500000 tweets processed...\n",
      "3600000 tweets processed...\n",
      "3700000 tweets processed...\n",
      "3800000 tweets processed...\n",
      "3900000 tweets processed...\n",
      "4000000 tweets processed...\n",
      "4100000 tweets processed...\n",
      "4200000 tweets processed...\n",
      "4300000 tweets processed...\n",
      "4400000 tweets processed...\n",
      "4500000 tweets processed...\n",
      "4600000 tweets processed...\n",
      "4700000 tweets processed...\n",
      "4800000 tweets processed...\n",
      "4900000 tweets processed...\n",
      "5000000 tweets processed...\n",
      "5100000 tweets processed...\n",
      "5200000 tweets processed...\n",
      "5300000 tweets processed...\n",
      "5400000 tweets processed...\n",
      "5500000 tweets processed...\n",
      "5600000 tweets processed...\n",
      "5700000 tweets processed...\n",
      "5800000 tweets processed...\n",
      "5900000 tweets processed...\n",
      "6000000 tweets processed...\n",
      "6100000 tweets processed...\n",
      "6200000 tweets processed...\n",
      "6300000 tweets processed...\n",
      "Number of missing words: 1165080\n"
     ]
    }
   ],
   "source": [
    "date_to_tweets = get_tweets('Delhi_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (1045, 319)\n",
      "Labels shape: (1045,)\n"
     ]
    }
   ],
   "source": [
    "features, labels = get_features_and_labels(date_to_tweets)\n",
    "print('Features shape:', features.shape)\n",
    "print('Labels shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true: monday max: blackfriday\n",
      "true: monday max: #friday\n",
      "true: monday max: #friday\n",
      "true: monday max: itsfriday\n",
      "true: monday max: #saturday\n",
      "true: monday max: throwbackthursday\n",
      "true: monday max: throwbackthursday\n",
      "true: monday max: supersaturday\n",
      "[[126   1   1   0   1  16   3]\n",
      " [  9  72   4   3  16  23  21]\n",
      " [  4   3  56   3  22  30  30]\n",
      " [  4   0   3  54  29  25  34]\n",
      " [  2   0   1   0 114  21  11]\n",
      " [  5   1   0   0   3 117  21]\n",
      " [  0   1   0   0   2  13 132]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = analyze_features(features, labels)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
