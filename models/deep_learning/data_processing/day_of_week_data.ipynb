{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.tokenize import casual_tokenize\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDINGS_PATH = '../data/'\n",
    "TWEET_PATH = '../data/city_tweets/'\n",
    "week_words = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of keywords:', 282)\n"
     ]
    }
   ],
   "source": [
    "embedding_keywords = set()\n",
    "word_dict = {}\n",
    "word_list = []\n",
    "word_embeddings = []\n",
    "with open(EMBEDDINGS_PATH + \"glove.twitter.27B.50d.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        word = line.split()[0]\n",
    "        if word.isalpha():\n",
    "            for day in week_words:\n",
    "                if day in word:\n",
    "                    embedding_keywords.add(len(word_embeddings))\n",
    "                    break\n",
    "        word_dict[word] = len(word_embeddings)\n",
    "        word_list.append(word)\n",
    "        word_embeddings.append([float(x) for x in line.split()[1:]])\n",
    "        while (len(word_embeddings[-1]) < 50):\n",
    "            word_embeddings[-1].append(0.0)\n",
    "        word_embeddings[-1] = word_embeddings[-1][:50]\n",
    "word_embeddings = np.array(word_embeddings)\n",
    "print(\"Number of keywords:\", len(embedding_keywords))\n",
    "keyword_to_feat_index = {}\n",
    "for i, idx in enumerate(embedding_keywords):\n",
    "    keyword_to_feat_index[idx] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_date(d):\n",
    "    year = int(d[:4])\n",
    "    month = int(d[5:7])\n",
    "    day = int(d[8:10])\n",
    "    return datetime.date(year, month, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweets(tweet_file, missing_words):\n",
    "    print(\"Processing:\", tweet_file)\n",
    "    date_to_tweets = defaultdict(list)\n",
    "    with open(TWEET_PATH + tweet_file) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        cnt = 0\n",
    "        for row in reader:\n",
    "            date = get_date(row['postedTime'])\n",
    "            tweet = ' '.join([word for word in casual_tokenize(row['tweet']) if '@' not in word and 'http' not in word])\n",
    "            tweet = tweet.replace('#', '').lower()\n",
    "            tweet_embedding = []\n",
    "            for word in tweet.split():\n",
    "                if word in word_dict:\n",
    "                    tweet_embedding.append(word_dict[word])\n",
    "                else:\n",
    "                    tweet_embedding.append(-1)\n",
    "                    missing_words.add(word)\n",
    "            date_to_tweets[date].append(tweet_embedding)\n",
    "            cnt += 1\n",
    "            if (cnt % 200000 == 0): \n",
    "                print(str(cnt),'tweets processed...')\n",
    "    return date_to_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_and_labels(date_to_tweets):\n",
    "    features, labels = [], []\n",
    "    for date in date_to_tweets:\n",
    "        hist = defaultdict(int)\n",
    "        for tweet in date_to_tweets[date]:\n",
    "            for idx in tweet:\n",
    "                if idx in embedding_keywords:\n",
    "                    hist[idx] += 1\n",
    "        feat = np.zeros(len(embedding_keywords), dtype=np.float32)\n",
    "        for idx, cnt in hist.items():\n",
    "            feat[keyword_to_feat_index[idx]] = cnt\n",
    "        features.append(feat/len(date_to_tweets[date]))\n",
    "        labels.append(date.weekday())\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_features(features, labels):\n",
    "    conf_matrix = np.zeros((7, 7), dtype=np.int32)\n",
    "    for i in range(features.shape[0]):\n",
    "        max_idx = np.argmax(features[i])\n",
    "        for key, val in keyword_to_feat_index.items():\n",
    "            if val == max_idx:\n",
    "                for j, day in enumerate(week_words):\n",
    "                    if day in word_list[key]:\n",
    "                        conf_matrix[labels[i]][j] += 1\n",
    "                        break\n",
    "                break\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Processing:', 'Kohima_tweets.csv')\n",
      "('Processing:', 'Cuttack_tweets.csv')\n",
      "('Processing:', 'Lucknow_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('400000', 'tweets processed...')\n",
      "('Processing:', 'Panchkula_tweets.csv')\n",
      "('Processing:', 'Siliguri_tweets.csv')\n",
      "('Processing:', 'Delhi_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('400000', 'tweets processed...')\n",
      "('600000', 'tweets processed...')\n",
      "('800000', 'tweets processed...')\n",
      "('1000000', 'tweets processed...')\n",
      "('1200000', 'tweets processed...')\n",
      "('1400000', 'tweets processed...')\n",
      "('1600000', 'tweets processed...')\n",
      "('1800000', 'tweets processed...')\n",
      "('2000000', 'tweets processed...')\n",
      "('2200000', 'tweets processed...')\n",
      "('2400000', 'tweets processed...')\n",
      "('2600000', 'tweets processed...')\n",
      "('2800000', 'tweets processed...')\n",
      "('3000000', 'tweets processed...')\n",
      "('3200000', 'tweets processed...')\n",
      "('3400000', 'tweets processed...')\n",
      "('3600000', 'tweets processed...')\n",
      "('3800000', 'tweets processed...')\n",
      "('4000000', 'tweets processed...')\n",
      "('4200000', 'tweets processed...')\n",
      "('4400000', 'tweets processed...')\n",
      "('4600000', 'tweets processed...')\n",
      "('4800000', 'tweets processed...')\n",
      "('5000000', 'tweets processed...')\n",
      "('5200000', 'tweets processed...')\n",
      "('5400000', 'tweets processed...')\n",
      "('5600000', 'tweets processed...')\n",
      "('5800000', 'tweets processed...')\n",
      "('6000000', 'tweets processed...')\n",
      "('6200000', 'tweets processed...')\n",
      "('Processing:', 'Ernakulam_tweets.csv')\n",
      "('Processing:', 'Ahmedabad_tweets.csv')\n",
      "('Processing:', 'Bathinda_tweets.csv')\n",
      "('Processing:', 'Sambalpur_tweets.csv')\n",
      "('Processing:', 'Ranchi_tweets.csv')\n",
      "('Processing:', 'Bengaluru_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('400000', 'tweets processed...')\n",
      "('600000', 'tweets processed...')\n",
      "('800000', 'tweets processed...')\n",
      "('1000000', 'tweets processed...')\n",
      "('1200000', 'tweets processed...')\n",
      "('1400000', 'tweets processed...')\n",
      "('1600000', 'tweets processed...')\n",
      "('1800000', 'tweets processed...')\n",
      "('2000000', 'tweets processed...')\n",
      "('2200000', 'tweets processed...')\n",
      "('2400000', 'tweets processed...')\n",
      "('2600000', 'tweets processed...')\n",
      "('2800000', 'tweets processed...')\n",
      "('3000000', 'tweets processed...')\n",
      "('3200000', 'tweets processed...')\n",
      "('3400000', 'tweets processed...')\n",
      "('3600000', 'tweets processed...')\n",
      "('Processing:', 'Shillong_tweets.csv')\n",
      "('Processing:', 'Kanpur_tweets.csv')\n",
      "('Processing:', 'Mumbai_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('400000', 'tweets processed...')\n",
      "('600000', 'tweets processed...')\n",
      "('800000', 'tweets processed...')\n",
      "('1000000', 'tweets processed...')\n",
      "('1200000', 'tweets processed...')\n",
      "('1400000', 'tweets processed...')\n",
      "('1600000', 'tweets processed...')\n",
      "('1800000', 'tweets processed...')\n",
      "('2000000', 'tweets processed...')\n",
      "('2200000', 'tweets processed...')\n",
      "('2400000', 'tweets processed...')\n",
      "('2600000', 'tweets processed...')\n",
      "('2800000', 'tweets processed...')\n",
      "('3000000', 'tweets processed...')\n",
      "('3200000', 'tweets processed...')\n",
      "('3400000', 'tweets processed...')\n",
      "('3600000', 'tweets processed...')\n",
      "('3800000', 'tweets processed...')\n",
      "('4000000', 'tweets processed...')\n",
      "('4200000', 'tweets processed...')\n",
      "('4400000', 'tweets processed...')\n",
      "('4600000', 'tweets processed...')\n",
      "('4800000', 'tweets processed...')\n",
      "('5000000', 'tweets processed...')\n",
      "('5200000', 'tweets processed...')\n",
      "('5400000', 'tweets processed...')\n",
      "('5600000', 'tweets processed...')\n",
      "('5800000', 'tweets processed...')\n",
      "('6000000', 'tweets processed...')\n",
      "('6200000', 'tweets processed...')\n",
      "('6400000', 'tweets processed...')\n",
      "('6600000', 'tweets processed...')\n",
      "('6800000', 'tweets processed...')\n",
      "('7000000', 'tweets processed...')\n",
      "('7200000', 'tweets processed...')\n",
      "('Processing:', 'Hyderabad_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('400000', 'tweets processed...')\n",
      "('600000', 'tweets processed...')\n",
      "('800000', 'tweets processed...')\n",
      "('1000000', 'tweets processed...')\n",
      "('Processing:', 'Chennai_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('Processing:', 'Thiruchirapalli_tweets.csv')\n",
      "('Processing:', 'Jabalpur_tweets.csv')\n",
      "('Processing:', 'Shimla_tweets.csv')\n",
      "('Processing:', 'Agra_tweets.csv')\n",
      "('Processing:', 'Vijaywada_tweets.csv')\n",
      "('Processing:', 'Bhopal_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('Processing:', 'Jammu_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('Processing:', 'Jaipur_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('400000', 'tweets processed...')\n",
      "('Processing:', 'Srinagar_tweets.csv')\n",
      "('Processing:', 'Rourkela_tweets.csv')\n",
      "('Processing:', 'Puducherry_tweets.csv')\n",
      "('Processing:', 'Ludhiana_tweets.csv')\n",
      "('Processing:', 'Nagpur_tweets.csv')\n",
      "('Processing:', 'Karnal_tweets.csv')\n",
      "('Processing:', 'Dehradun_tweets.csv')\n",
      "('Processing:', 'Patna_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('Processing:', 'Dindigul_tweets.csv')\n",
      "('Processing:', 'Rajkot_tweets.csv')\n",
      "('Processing:', 'Bhagalpur_tweets.csv')\n",
      "('Processing:', 'Guwahati_tweets.csv')\n",
      "('Processing:', 'Raipur_tweets.csv')\n",
      "('Processing:', 'Port Blair_tweets.csv')\n",
      "('Processing:', 'Agartala_tweets.csv')\n",
      "('Processing:', 'Trivandrum_tweets.csv')\n",
      "('Processing:', 'Jodhpur_tweets.csv')\n",
      "('Processing:', 'Gwalior_tweets.csv')\n",
      "('Processing:', 'Amritsar_tweets.csv')\n",
      "('Processing:', 'Itanagar_tweets.csv')\n",
      "('Processing:', 'Bhubaneshwar_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('Processing:', 'Kolkata_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('400000', 'tweets processed...')\n",
      "('600000', 'tweets processed...')\n",
      "('800000', 'tweets processed...')\n",
      "('1000000', 'tweets processed...')\n",
      "('1200000', 'tweets processed...')\n",
      "('1400000', 'tweets processed...')\n",
      "('1600000', 'tweets processed...')\n",
      "('1800000', 'tweets processed...')\n",
      "('Processing:', 'Panaji_tweets.csv')\n",
      "('Processing:', 'Gurgaon_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('400000', 'tweets processed...')\n",
      "('600000', 'tweets processed...')\n",
      "('800000', 'tweets processed...')\n",
      "('1000000', 'tweets processed...')\n",
      "('Processing:', 'Hisar_tweets.csv')\n",
      "('Processing:', 'Mandi_tweets.csv')\n",
      "('Processing:', 'Aizwal_tweets.csv')\n",
      "('Processing:', 'Kozhidoke_tweets.csv')\n",
      "('Processing:', 'Indore_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('Processing:', 'Kota_tweets.csv')\n",
      "('Processing:', 'Chandigarh_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('Processing:', 'Dharwad_tweets.csv')\n",
      "('Processing:', 'Varanasi_tweets.csv')\n",
      "('Processing:', 'Dimapur_tweets.csv')\n",
      "('Features shape:', (42742, 282))\n",
      "('Labels shape:', (42742,))\n",
      "('Number of missing words:', 3418650)\n"
     ]
    }
   ],
   "source": [
    "missing_words = set()\n",
    "features, labels = [], []\n",
    "for tweets_file in os.listdir(TWEET_PATH):\n",
    "    date_to_tweets = get_tweets(tweets_file, missing_words)\n",
    "    cur_features, cur_labels = get_features_and_labels(date_to_tweets)\n",
    "    features += cur_features\n",
    "    labels += cur_labels\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "conf_matrix = analyze_features(features, labels)\n",
    "print(conf_matrix)\n",
    "print(\"Number of missing words:\", len(missing_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(325)\n",
    "train_idx = set(list(np.random.choice(features.shape[0], 35000, replace=False)))\n",
    "train_file = open(EMBEDDINGS_PATH + \"dow_train_feat.txt\", \"w\")\n",
    "val_file = open(EMBEDDINGS_PATH + \"dow_val_feat.txt\", \"w\")\n",
    "for i in range(features.shape[0]):\n",
    "    if i in train_idx:\n",
    "        for j in range(features.shape[1]):\n",
    "            train_file.write(\"%.10f \" % features[i][j])\n",
    "        train_file.write('%d\\n' % labels[i])\n",
    "    else:\n",
    "        for j in range(features.shape[1]):\n",
    "            val_file.write(\"%.10f \" % features[i][j])\n",
    "        val_file.write('%d\\n' % labels[i])\n",
    "train_file.close()\n",
    "val_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Processing:', 'Delhi_tweets.csv')\n",
      "('200000', 'tweets processed...')\n",
      "('400000', 'tweets processed...')\n",
      "('600000', 'tweets processed...')\n",
      "('800000', 'tweets processed...')\n",
      "('1000000', 'tweets processed...')\n",
      "('1200000', 'tweets processed...')\n",
      "('1400000', 'tweets processed...')\n",
      "('1600000', 'tweets processed...')\n",
      "('1800000', 'tweets processed...')\n",
      "('2000000', 'tweets processed...')\n",
      "('2200000', 'tweets processed...')\n",
      "('2400000', 'tweets processed...')\n",
      "('2600000', 'tweets processed...')\n",
      "('2800000', 'tweets processed...')\n",
      "('3000000', 'tweets processed...')\n",
      "('3200000', 'tweets processed...')\n",
      "('3400000', 'tweets processed...')\n",
      "('3600000', 'tweets processed...')\n",
      "('3800000', 'tweets processed...')\n",
      "('4000000', 'tweets processed...')\n",
      "('4200000', 'tweets processed...')\n",
      "('4400000', 'tweets processed...')\n",
      "('4600000', 'tweets processed...')\n",
      "('4800000', 'tweets processed...')\n",
      "('5000000', 'tweets processed...')\n",
      "('5200000', 'tweets processed...')\n",
      "('5400000', 'tweets processed...')\n",
      "('5600000', 'tweets processed...')\n",
      "('5800000', 'tweets processed...')\n",
      "('6000000', 'tweets processed...')\n",
      "('6200000', 'tweets processed...')\n",
      "('Features shape:', (1045, 282))\n",
      "('Labels shape:', (1045,))\n",
      "[[128   1   1   0   1  16   1]\n",
      " [  4  84   4   1  14  21  21]\n",
      " [  4   3  71   0  17  32  24]\n",
      " [  5   0   1  59  27  24  35]\n",
      " [  1   0   1   0 120  21   7]\n",
      " [  5   1   0   0   3 128  11]\n",
      " [  0   1   0   0   2  13 132]]\n"
     ]
    }
   ],
   "source": [
    "delhi_date_to_tweets = get_tweets(\"Delhi_tweets.csv\", missing_words)\n",
    "dfeatures, dlabels = get_features_and_labels(delhi_date_to_tweets)\n",
    "dfeatures = np.array(dfeatures)\n",
    "dlabels = np.array(dlabels)\n",
    "print(\"Features shape:\", dfeatures.shape)\n",
    "print(\"Labels shape:\", dlabels.shape)\n",
    "dconf_matrix = analyze_features(dfeatures, dlabels)\n",
    "print(dconf_matrix)\n",
    "output_file = open(EMBEDDINGS_PATH + \"dow_delhi_feat.txt\", \"w\")\n",
    "for i in range(dfeatures.shape[0]):\n",
    "    for j in range(dfeatures.shape[1]):\n",
    "        output_file.write(\"%.10f \" % dfeatures[i][j])\n",
    "    output_file.write('%d\\n' % dlabels[i])\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
