{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import dateutil.parser\n",
    "import datetime\n",
    "import HTMLParser\n",
    "from nearest_words import read_file, word_to_embedding\n",
    "import nltk\n",
    "from nltk.tokenize import casual_tokenize\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Kohima_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Cuttack_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "Reading Lucknow_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "400000 tweets processed...\n",
      "Reading Panchkula_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Siliguri_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Delhi_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "400000 tweets processed...\n",
      "500000 tweets processed...\n",
      "600000 tweets processed...\n",
      "700000 tweets processed...\n",
      "800000 tweets processed...\n",
      "900000 tweets processed...\n",
      "1000000 tweets processed...\n",
      "1100000 tweets processed...\n",
      "1200000 tweets processed...\n",
      "1300000 tweets processed...\n",
      "1400000 tweets processed...\n",
      "1500000 tweets processed...\n",
      "1600000 tweets processed...\n",
      "1700000 tweets processed...\n",
      "1800000 tweets processed...\n",
      "1900000 tweets processed...\n",
      "2000000 tweets processed...\n",
      "2100000 tweets processed...\n",
      "2200000 tweets processed...\n",
      "2300000 tweets processed...\n",
      "2400000 tweets processed...\n",
      "2500000 tweets processed...\n",
      "2600000 tweets processed...\n",
      "2700000 tweets processed...\n",
      "2800000 tweets processed...\n",
      "2900000 tweets processed...\n",
      "3000000 tweets processed...\n",
      "3100000 tweets processed...\n",
      "3200000 tweets processed...\n",
      "3300000 tweets processed...\n",
      "3400000 tweets processed...\n",
      "3500000 tweets processed...\n",
      "3600000 tweets processed...\n",
      "3700000 tweets processed...\n",
      "3800000 tweets processed...\n",
      "3900000 tweets processed...\n",
      "4000000 tweets processed...\n",
      "4100000 tweets processed...\n",
      "4200000 tweets processed...\n",
      "4300000 tweets processed...\n",
      "4400000 tweets processed...\n",
      "4500000 tweets processed...\n",
      "4600000 tweets processed...\n",
      "4700000 tweets processed...\n",
      "4800000 tweets processed...\n",
      "4900000 tweets processed...\n",
      "5000000 tweets processed...\n",
      "5100000 tweets processed...\n",
      "5200000 tweets processed...\n",
      "5300000 tweets processed...\n",
      "5400000 tweets processed...\n",
      "5500000 tweets processed...\n",
      "5600000 tweets processed...\n",
      "5700000 tweets processed...\n",
      "5800000 tweets processed...\n",
      "5900000 tweets processed...\n",
      "6000000 tweets processed...\n",
      "6100000 tweets processed...\n",
      "6200000 tweets processed...\n",
      "6300000 tweets processed...\n",
      "Reading Ernakulam_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Ahmedabad_tweets.csv...\n",
      "Reading Bathinda_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Sambalpur_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Ranchi_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Bengaluru_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "400000 tweets processed...\n",
      "500000 tweets processed...\n",
      "600000 tweets processed...\n",
      "700000 tweets processed...\n",
      "800000 tweets processed...\n",
      "900000 tweets processed...\n",
      "1000000 tweets processed...\n",
      "1100000 tweets processed...\n",
      "1200000 tweets processed...\n",
      "1300000 tweets processed...\n",
      "1400000 tweets processed...\n"
     ]
    }
   ],
   "source": [
    "PATH = '../../../data_utils/city_tweets/'\n",
    "f = open(\"weekly_tweet_cnts.txt\", \"w\")\n",
    "for tweets_file in os.listdir(PATH):\n",
    "    print 'Reading ' + tweets_file + '...'\n",
    "    city = tweets_file.split('_')[0]\n",
    "    tweets_processed = 0\n",
    "    f.write(city + \"\\n\")\n",
    "    \n",
    "    cur_date = datetime.date(2014, 1, 1)\n",
    "    cur_cnt = 0\n",
    "    with open(PATH + tweets_file) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if (tweets_processed % 1e5 == 0): print str(tweets_processed) + ' tweets processed...'\n",
    "            tweets_processed += 1\n",
    "            tweet_date = datetime.date(int(row['postedTime'][0:4]), int(row['postedTime'][5:7]), int(row['postedTime'][8:10]))\n",
    "            if tweet_date - cur_date < datetime.timedelta(days = 7):\n",
    "                cur_cnt += 1\n",
    "            else:\n",
    "                #print \"week of \", str(cur_date),  \" count: \", str(cur_cnt)\n",
    "                f.write(\"week of \" + str(cur_date) + \" count: \" + str(cur_cnt) + \"\\n\")\n",
    "                cur_date = tweet_date\n",
    "                cur_cnt = 1\n",
    "        #print \"week of \", str(cur_date),  \" count: \", str(cur_cnt)\n",
    "        f.write(\"week of \" + str(cur_date) + \" count: \" + str(cur_cnt) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
