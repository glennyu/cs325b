{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total weeks: 0\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import dateutil.parser\n",
    "import datetime\n",
    "import HTMLParser\n",
    "from nearest_words import read_file, word_to_embedding\n",
    "import nltk\n",
    "from nltk.tokenize import casual_tokenize\n",
    "import numpy as np\n",
    "import os\n",
    "from contains_city import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Kohima_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Lucknow_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "400000 tweets processed...\n",
      "Reading Panchkula_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Siliguri_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Ernakulam_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Ahmedabad_tweets.csv...\n",
      "Reading Bathinda_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Sambalpur_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Ranchi_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Kanpur_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "Reading Mumbai_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "400000 tweets processed...\n",
      "500000 tweets processed...\n",
      "600000 tweets processed...\n",
      "700000 tweets processed...\n",
      "800000 tweets processed...\n",
      "900000 tweets processed...\n",
      "1000000 tweets processed...\n",
      "1100000 tweets processed...\n",
      "1200000 tweets processed...\n",
      "1300000 tweets processed...\n",
      "1400000 tweets processed...\n",
      "1500000 tweets processed...\n",
      "1600000 tweets processed...\n",
      "1700000 tweets processed...\n",
      "1800000 tweets processed...\n",
      "1900000 tweets processed...\n",
      "2000000 tweets processed...\n",
      "2100000 tweets processed...\n",
      "2200000 tweets processed...\n",
      "2300000 tweets processed...\n",
      "2400000 tweets processed...\n",
      "2500000 tweets processed...\n",
      "2600000 tweets processed...\n",
      "2700000 tweets processed...\n",
      "2800000 tweets processed...\n",
      "2900000 tweets processed...\n",
      "3000000 tweets processed...\n",
      "3100000 tweets processed...\n",
      "3200000 tweets processed...\n",
      "3300000 tweets processed...\n",
      "3400000 tweets processed...\n",
      "3500000 tweets processed...\n",
      "3600000 tweets processed...\n",
      "3700000 tweets processed...\n",
      "3800000 tweets processed...\n",
      "3900000 tweets processed...\n",
      "4000000 tweets processed...\n",
      "4100000 tweets processed...\n",
      "4200000 tweets processed...\n",
      "4300000 tweets processed...\n",
      "4400000 tweets processed...\n",
      "4500000 tweets processed...\n",
      "4600000 tweets processed...\n",
      "4700000 tweets processed...\n",
      "4800000 tweets processed...\n",
      "4900000 tweets processed...\n",
      "5000000 tweets processed...\n",
      "5100000 tweets processed...\n",
      "5200000 tweets processed...\n",
      "5300000 tweets processed...\n",
      "5400000 tweets processed...\n",
      "5500000 tweets processed...\n",
      "5600000 tweets processed...\n",
      "5700000 tweets processed...\n",
      "5800000 tweets processed...\n",
      "5900000 tweets processed...\n",
      "6000000 tweets processed...\n",
      "6100000 tweets processed...\n",
      "6200000 tweets processed...\n",
      "6300000 tweets processed...\n",
      "6400000 tweets processed...\n",
      "6500000 tweets processed...\n",
      "6600000 tweets processed...\n",
      "6700000 tweets processed...\n",
      "6800000 tweets processed...\n",
      "6900000 tweets processed...\n",
      "7000000 tweets processed...\n",
      "7100000 tweets processed...\n",
      "7200000 tweets processed...\n",
      "Reading Hyderabad_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "400000 tweets processed...\n",
      "500000 tweets processed...\n",
      "600000 tweets processed...\n",
      "700000 tweets processed...\n",
      "800000 tweets processed...\n",
      "900000 tweets processed...\n",
      "1000000 tweets processed...\n",
      "Reading Jabalpur_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Agra_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Bhopal_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "Reading Jammu_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "Reading Jaipur_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "400000 tweets processed...\n",
      "Reading Ludhiana_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "Reading Nagpur_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "Reading Karnal_tweets.csv...\n",
      "Reading Patna_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "Reading Dindigul_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Rajkot_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Bhagalpur_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Guwahati_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "Reading Raipur_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Port Blair_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Jodhpur_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "Reading Amritsar_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Panaji_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Gurgaon_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "400000 tweets processed...\n",
      "500000 tweets processed...\n",
      "600000 tweets processed...\n",
      "700000 tweets processed...\n",
      "800000 tweets processed...\n",
      "900000 tweets processed...\n",
      "1000000 tweets processed...\n",
      "Reading Hisar_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Mandi_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Indore_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "Reading Kota_tweets.csv...\n",
      "0 tweets processed...\n",
      "Reading Chandigarh_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "Reading Varanasi_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n"
     ]
    }
   ],
   "source": [
    "PATH = '../../../data_utils/city_tweets/'\n",
    "f = open(\"weekly_tweet_cnts.txt\", \"w\")\n",
    "city_to_weekly_prices = get_city_to_weeks()\n",
    "for tweets_file in os.listdir(PATH):\n",
    "    city = tweets_file.split('_')[0]\n",
    "    if city.lower() not in city_to_weekly_prices.keys():\n",
    "        continue\n",
    "    print 'Reading ' + tweets_file + '...'\n",
    "    tweets_processed = 0\n",
    "    f.write(city + \"\\n\")\n",
    "    \n",
    "    cur_date = datetime.date(2014, 1, 1)\n",
    "    cur_cnt = {}\n",
    "    p = city_to_weekly_prices[city.lower()]\n",
    "    for week in p:\n",
    "        cur_cnt[datetime.date(week[0], week[1], week[2])] = 0\n",
    "        \n",
    "    with open(PATH + tweets_file) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if (tweets_processed % 1e5 == 0): print str(tweets_processed) + ' tweets processed...'\n",
    "            tweets_processed += 1\n",
    "            tweet_date = datetime.date(int(row['postedTime'][0:4]), int(row['postedTime'][5:7]), int(row['postedTime'][8:10]))\n",
    "            for week in cur_cnt.keys():\n",
    "                if week >= tweet_date and week - tweet_date < datetime.timedelta(days = 7):\n",
    "                    cur_cnt[week] += 1\n",
    "                    break\n",
    "    for key, val in cur_cnt.iteritems():\n",
    "        f.write(str(key) + \": \" + str(val) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
