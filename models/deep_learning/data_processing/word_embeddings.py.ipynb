{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import dateutil.parser\n",
    "import HTMLParser\n",
    "import nltk\n",
    "from nltk.tokenize import casual_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "PATH = '../../../../'\n",
    "EMBEDDINGS_FILE = 'glove.twitter.27B/glove.twitter.27B.50d.txt'\n",
    "MISSING_WORDS_FILE = 'missing_words.txt'\n",
    "WORDS_FILE = 'words.txt'\n",
    "TWEET_CNTS_FILE = 'tweet_counts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "word_to_idx = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_words_file():\n",
    "    print 'Begin creating words file...'\n",
    "    with open(WORDS_FILE, 'w') as wf:\n",
    "        with open(PATH + EMBEDDINGS_FILE, 'r') as ef:\n",
    "            idx = 0\n",
    "            for line in ef:\n",
    "                word = line.split()[0]\n",
    "                wf.write(word + '\\n')\n",
    "                word_to_idx[word] = idx\n",
    "                idx += 1\n",
    "    print 'Finished creating words file!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_words_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_tweet_counts(month_to_tweets):\n",
    "    with open(TWEET_CNTS_FILE, 'w') as f:\n",
    "        f.write(','.join([str(len(month_to_tweets[month])) for month in month_to_tweets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_missing_words(missing_words):\n",
    "    with open(MISSING_WORDS_FILE, 'w') as f:\n",
    "        for word in missing_words:\n",
    "            f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def output_embeddings(month_to_tweets, title):\n",
    "    print 'Begin outputting word embeddings...'\n",
    "    for month in month_to_tweets:\n",
    "        with open(str(month) + '_embeddings_' + title + '.csv', 'w') as output:\n",
    "            for tweet in month_to_tweets[month]:\n",
    "                output.write(','.join([str(num) for num in tweet]) + '\\n')\n",
    "    print 'Finished outputting word embeddings!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_word_embeddings():\n",
    "    print 'Begin getting word embeddings...'\n",
    "    tweets_file = 'Delhi_tweets.csv'\n",
    "    title = 'delhi'\n",
    "    cnt = 0\n",
    "    with open(PATH + tweets_file) as csvfile:\n",
    "        month_to_tweets = defaultdict(list)\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        missing_words = set()\n",
    "        missing_words_cnt = 0\n",
    "        for row in reader:\n",
    "            if (cnt % 100000 == 0): print str(cnt) + ' tweets processed...'\n",
    "            cnt += 1\n",
    "            date = dateutil.parser.parse(row['postedTime'])\n",
    "            month_idx = (date.year - 2014)*12 + (date.month - 1)\n",
    "            tweet = ' '.join([word for word in casual_tokenize(row['tweet']) \n",
    "                              if '@' not in word and 'http' not in word and '#' not in word])\n",
    "            tweet = tweet.lower()\n",
    "            tweet_embedding = []\n",
    "            for word in tweet.split():\n",
    "                if (word in word_to_idx):\n",
    "                    tweet_embedding.append(word_to_idx[word])\n",
    "                else:\n",
    "                    missing_words_cnt += 1\n",
    "                    missing_words.add(word)\n",
    "            if (len(tweet_embedding) > 0): month_to_tweets[month_idx].append(tweet_embedding)\n",
    "            output_embeddings(month_to_tweets, title)\n",
    "            output_tweet_counts(month_to_tweets)\n",
    "            output_missing_words(missing_words)\n",
    "    print 'Finished getting word embeddings!'\n",
    "    print 'Number of missing words: ' + str(missing_words_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_word_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
