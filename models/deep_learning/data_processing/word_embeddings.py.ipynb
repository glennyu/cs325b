{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import dateutil.parser\n",
    "import HTMLParser\n",
    "from nearest_words import read_file, word_to_embedding\n",
    "import nltk\n",
    "from nltk.tokenize import casual_tokenize\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "PATH = '../data/'\n",
    "EMBEDDINGS_FILE = 'glove.twitter.27B.50d.txt'\n",
    "MISSING_WORDS_FILE = 'missing_words.txt'\n",
    "WORDS_FILE = 'words.txt'\n",
    "TWEET_CNTS_FILE = 'tweet_counts.txt'\n",
    "MIN_DIST = 4.5\n",
    "NUM_MONTHS = 35\n",
    "MIN_TWEETS = 100 # minimum number of tweets required per city-month\n",
    "K = 50 # number of tweets per batch\n",
    "NUM_RESAMPLES = 8 # number of times to resample from city-month tweets to generate batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "word_to_idx = dict() # word to index in glove embeddings file\n",
    "city_to_changes = defaultdict(lambda: defaultdict(lambda: [0, 0])) # city to month to [trend, spike]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_words_file():\n",
    "    print 'Begin creating words file...'\n",
    "    with open(WORDS_FILE, 'w') as wf:\n",
    "        with open(PATH + EMBEDDINGS_FILE, 'r') as ef:\n",
    "            idx = 0\n",
    "            for line in ef:\n",
    "                word = line.split()[0]\n",
    "                wf.write(word + '\\n')\n",
    "                word_to_idx[word] = idx\n",
    "                idx += 1\n",
    "    print 'Finished creating words file!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin creating words file...\n",
      "Finished creating words file!\n"
     ]
    }
   ],
   "source": [
    "create_words_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_tweet_counts(month_to_tweets):\n",
    "    with open(TWEET_CNTS_FILE, 'w') as f:\n",
    "        f.write(','.join([str(len(month_to_tweets[month])) for month in month_to_tweets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_missing_words(missing_words):\n",
    "    with open(MISSING_WORDS_FILE, 'w') as f:\n",
    "        for word in missing_words:\n",
    "            f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def output_embeddings(month_to_tweets, city):\n",
    "    print 'Begin outputting word embeddings...'\n",
    "    for month in month_to_tweets:\n",
    "        if (len(month_to_tweets[month]) >= MIN_TWEETS):\n",
    "            with open('%s/embeddings/%s_%d_embeddings.csv' % (PATH[:-1], city, month), 'w') as output:\n",
    "                for tweet in month_to_tweets[month]:\n",
    "                    output.write(','.join([str(num) for num in tweet]) + '\\n')\n",
    "    print 'Finished outputting word embeddings!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_word_embeddings():\n",
    "    print 'Begin getting word embeddings...'\n",
    "    tweets_file = 'Delhi_tweets.csv'\n",
    "    title = 'delhi'\n",
    "    cnt = 0\n",
    "    with open(PATH + tweets_file) as csvfile:\n",
    "        month_to_tweets = defaultdict(list)\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        missing_words = set()\n",
    "        missing_words_cnt = 0\n",
    "        for row in reader:\n",
    "            if (cnt % 100000 == 0): print str(cnt) + ' tweets processed...'\n",
    "            cnt += 1\n",
    "            date = dateutil.parser.parse(row['postedTime'])\n",
    "            month_idx = (date.year - 2014)*12 + (date.month - 1)\n",
    "            tweet = ' '.join([word for word in casual_tokenize(row['tweet']) \n",
    "                              if '@' not in word and 'http' not in word and '#' not in word])\n",
    "            tweet = tweet.lower()\n",
    "            tweet_embedding = []\n",
    "            for word in tweet.split():\n",
    "                if (word in word_to_idx):\n",
    "                    tweet_embedding.append(word_to_idx[word])\n",
    "                else:\n",
    "                    missing_words_cnt += 1\n",
    "                    missing_words.add(word)\n",
    "            if (len(tweet_embedding) > 0): month_to_tweets[month_idx].append(tweet_embedding)\n",
    "            output_embeddings(month_to_tweets, title)\n",
    "            output_tweet_counts(month_to_tweets)\n",
    "            output_missing_words(missing_words)\n",
    "    print 'Finished getting word embeddings!'\n",
    "    print 'Number of missing words: ' + str(missing_words_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_word_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5ab23519700b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_word_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_word_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "get_word_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_deviations():\n",
    "    print 'Getting deviations...'\n",
    "    with open(PATH + 'India_Onion_Prices_Vector.csv') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader, None)\n",
    "        for row in reader:\n",
    "            city = row[0]\n",
    "            trends = row[2:NUM_MONTHS + 2]\n",
    "            spikes = row[NUM_MONTHS + 3:2*NUM_MONTHS + 3]\n",
    "            for i in range(NUM_MONTHS):\n",
    "                trend = int(trends[i]) + 1 if (trends[i] != 'NA') else -1\n",
    "                spike = int(spikes[i]) if (spikes[i] != 'NA') else -1\n",
    "                city_to_changes[city][i] = [trend, spike]\n",
    "    print ' -done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_relevant(tweet, keyword_embedding):\n",
    "    for word in tweet.split():\n",
    "        if (word in word_to_embedding):\n",
    "            embedding = np.array(word_to_embedding[word])\n",
    "            dist = np.linalg.norm(embedding - keyword_embedding)\n",
    "            if (dist <= MIN_DIST):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_batches(month_to_tweets, city):\n",
    "    num_batches = 0\n",
    "    for month in month_to_tweets:\n",
    "        tweet_count = len(month_to_tweets[month])\n",
    "        trend = city_to_changes[city][month][0]\n",
    "        spike = city_to_changes[city][month][1]\n",
    "        if (tweet_count >= MIN_TWEETS and trend != -1 and spike != -1):\n",
    "            num_batches += create_batches(tweet_count, city, month, trend, spike)\n",
    "    return num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_batches(tweet_count, city_name, month_idx, trend, spike):\n",
    "    np.random.seed(10)\n",
    "    \n",
    "    n = tweet_count\n",
    "    rand_seq = []\n",
    "    for i in range(NUM_RESAMPLES):\n",
    "        cur = [str(num) for num in np.random.choice(n, n, replace=False)]\n",
    "        remaining = K - (n % K) # used to make the length of rand_seq a multiple of K\n",
    "        cur += [str(num) for num in np.random.choice(n, remaining, replace=False)]\n",
    "        rand_seq += cur\n",
    "\n",
    "    num_batches = len(rand_seq) / K\n",
    "    folder = 'batches'\n",
    "    output_file = '%s%s/%s_%s_batch.txt' % (PATH, folder, city_name, str(month_idx))\n",
    "    with open(output_file, 'w') as output:\n",
    "        output.write('%d,%d\\n' % (trend, spike))\n",
    "        for i in range(num_batches):\n",
    "            suffix = '\\n'\n",
    "            if (i == num_batches - 1): suffix = ''\n",
    "            output.write('\\t'.join(rand_seq[i * K : (i + 1) * K]) + suffix)\n",
    "    \n",
    "    with open(PATH + 'batch_counts.txt', 'a') as f:\n",
    "        f.write('%s,%d,%d\\n' % (city_name, month_idx, num_batches))\n",
    "        \n",
    "    return num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_relevant_word_embeddings(keyword):\n",
    "    keyword_embedding = np.array(word_to_embedding[keyword])\n",
    "    total_batches = 0\n",
    "    \n",
    "    for tweets_file in os.listdir(PATH):\n",
    "        if ('.csv' not in tweets_file) or ('India_Onion_Prices' in tweets_file): continue\n",
    "        if ('Mumbai' not in tweets_file): continue\n",
    "        print 'Reading ' + tweets_file + '...'\n",
    "        city = tweets_file.split('_')[0]\n",
    "        tweets_processed = 0\n",
    "        month_to_tweets = defaultdict(list)\n",
    "\n",
    "        with open(PATH + tweets_file) as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                if (tweets_processed % 1e5 == 0): print str(tweets_processed) + ' tweets processed...'\n",
    "                tweets_processed += 1\n",
    "                date = dateutil.parser.parse(row['postedTime'])\n",
    "                month_idx = (date.year - 2014)*12 + (date.month - 1)\n",
    "                tweet = ' '.join([word for word in casual_tokenize(row['tweet']) \n",
    "                                  if '@' not in word and 'http' not in word and '#' not in word])\n",
    "                tweet = tweet.lower()\n",
    "                tweet_embedding = []\n",
    "                if (is_relevant(tweet, keyword_embedding)):\n",
    "                    for word in tweet.split():\n",
    "                        if (word in word_to_idx):\n",
    "                            tweet_embedding.append(word_to_idx[word])\n",
    "                if (len(tweet_embedding) > 0): \n",
    "                    month_to_tweets[month_idx].append(tweet_embedding)\n",
    "                \n",
    "        output_embeddings(month_to_tweets, city)\n",
    "        \n",
    "#         print 'Outputting batches...'\n",
    "#         batches_created = output_batches(month_to_tweets, city)\n",
    "        \n",
    "#         total_batches += batches_created\n",
    "#         print 'Batches so far: %d' % total_batches\n",
    "#         print ' -done.'\n",
    "    \n",
    "#     print 'Total batches: %d' % total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file...\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      " -done\n"
     ]
    }
   ],
   "source": [
    "# Reads embeddings files and creates word_to_embedding map\n",
    "read_file(PATH + EMBEDDINGS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting deviations...\n",
      " -done.\n"
     ]
    }
   ],
   "source": [
    "# Stores all price changes and spikes in city_to_changes\n",
    "get_deviations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Mumbai_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "400000 tweets processed...\n",
      "500000 tweets processed...\n",
      "600000 tweets processed...\n",
      "700000 tweets processed...\n",
      "800000 tweets processed...\n",
      "900000 tweets processed...\n",
      "1000000 tweets processed...\n",
      "1100000 tweets processed...\n",
      "1200000 tweets processed...\n",
      "1300000 tweets processed...\n",
      "1400000 tweets processed...\n",
      "1500000 tweets processed...\n",
      "1600000 tweets processed...\n",
      "1700000 tweets processed...\n",
      "1800000 tweets processed...\n",
      "1900000 tweets processed...\n",
      "2000000 tweets processed...\n",
      "2100000 tweets processed...\n",
      "2200000 tweets processed...\n",
      "2300000 tweets processed...\n",
      "2400000 tweets processed...\n",
      "2500000 tweets processed...\n",
      "2600000 tweets processed...\n",
      "2700000 tweets processed...\n",
      "2800000 tweets processed...\n",
      "2900000 tweets processed...\n",
      "3000000 tweets processed...\n",
      "3100000 tweets processed...\n",
      "3200000 tweets processed...\n",
      "3300000 tweets processed...\n",
      "3400000 tweets processed...\n",
      "3500000 tweets processed...\n",
      "3600000 tweets processed...\n",
      "3700000 tweets processed...\n",
      "3800000 tweets processed...\n",
      "3900000 tweets processed...\n",
      "4000000 tweets processed...\n",
      "4100000 tweets processed...\n",
      "4200000 tweets processed...\n",
      "4300000 tweets processed...\n",
      "4400000 tweets processed...\n",
      "4500000 tweets processed...\n",
      "4600000 tweets processed...\n",
      "4700000 tweets processed...\n",
      "4800000 tweets processed...\n",
      "4900000 tweets processed...\n",
      "5000000 tweets processed...\n",
      "5100000 tweets processed...\n",
      "5200000 tweets processed...\n",
      "5300000 tweets processed...\n",
      "5400000 tweets processed...\n",
      "5500000 tweets processed...\n",
      "5600000 tweets processed...\n",
      "5700000 tweets processed...\n",
      "5800000 tweets processed...\n",
      "5900000 tweets processed...\n",
      "6000000 tweets processed...\n",
      "6100000 tweets processed...\n",
      "6200000 tweets processed...\n",
      "6300000 tweets processed...\n",
      "6400000 tweets processed...\n",
      "6500000 tweets processed...\n",
      "6600000 tweets processed...\n",
      "6700000 tweets processed...\n",
      "6800000 tweets processed...\n",
      "6900000 tweets processed...\n",
      "7000000 tweets processed...\n",
      "7100000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "../data/embeddings\n",
      "Finished outputting word embeddings!\n"
     ]
    }
   ],
   "source": [
    "# Creates all tweet embeddings related to given parameter and outputs batches\n",
    "get_relevant_word_embeddings('onion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets: 638606\n",
      "city: Chandigarh, batches: 1328\n",
      "city: Gurgaon, batches: 4040\n",
      "city: Chennai, batches: 1992\n",
      "city: Lucknow, batches: 1376\n",
      "city: Hyderabad, batches: 4128\n",
      "city: Delhi, batches: 24560\n",
      "city: Mumbai, batches: 38520\n",
      "city: Bengaluru, batches: 18424\n",
      "city: Jaipur, batches: 1264\n",
      "city: Kolkata, batches: 6952\n",
      "Total number of batches: 102584\n"
     ]
    }
   ],
   "source": [
    "city_to_tweets = defaultdict(lambda: [0 for i in range(NUM_MONTHS)])\n",
    "\n",
    "# Read embeddings files\n",
    "def read_embeddings_files():\n",
    "    path = '../data/embeddings/'\n",
    "    total = 0\n",
    "    for embeddings_file in os.listdir(path):\n",
    "        filename = embeddings_file.split('_')\n",
    "        city = filename[0]\n",
    "        month = int(filename[1])\n",
    "        with open(path + embeddings_file, 'r') as csvfile:\n",
    "            num_tweets = sum(1 for row in csv.reader(csvfile))\n",
    "            city_to_tweets[city][month] += num_tweets\n",
    "            total += num_tweets\n",
    "    print 'Total number of tweets: %d' % total\n",
    "            \n",
    "# Outputs batches by reading embeddings files to get tweet counts\n",
    "def output_batches_1():\n",
    "    num_batches = 0\n",
    "    for city in city_to_tweets:\n",
    "        city_batches = 0\n",
    "        for month in range(NUM_MONTHS):\n",
    "            tweet_count = city_to_tweets[city][month]\n",
    "            trend = city_to_changes[city][month][0]\n",
    "            spike = city_to_changes[city][month][1]\n",
    "            if (tweet_count >= MIN_TWEETS and trend != -1 and spike != -1):\n",
    "                batches = create_batches(tweet_count, city, month, trend, spike)\n",
    "                city_batches += batches\n",
    "        \n",
    "        num_batches += city_batches\n",
    "        print 'city: %s, batches: %d' % (city, city_batches)\n",
    "        \n",
    "    print 'Total number of batches: %d' % num_batches\n",
    "\n",
    "read_embeddings_files()\n",
    "output_batches_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
