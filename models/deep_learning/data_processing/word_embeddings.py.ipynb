{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from collections import defaultdict\n",
    "from contains_city import *\n",
    "import csv\n",
    "import dateutil.parser\n",
    "import HTMLParser\n",
    "from nearest_words import read_file, word_to_embedding\n",
    "import nltk\n",
    "from nltk.tokenize import casual_tokenize\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "PATH = '../data/'\n",
    "TWEET_PATH = '../../../data_utils/city_tweets/'\n",
    "EMBEDDINGS_FILE = 'glove.twitter.27B.50d.txt'\n",
    "MISSING_WORDS_FILE = 'missing_words.txt'\n",
    "WORDS_FILE = 'words.txt'\n",
    "TWEET_CNTS_FILE = 'tweet_counts.txt'\n",
    "MIN_DIST = 4.5\n",
    "NUM_MONTHS = 35\n",
    "MIN_TWEETS = 200 # minimum number of tweets required per city-month\n",
    "K = 50 # number of tweets per batch\n",
    "NUM_RESAMPLES = 5 # number of times to resample from city-month tweets to generate batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "word_to_idx = dict() # word to index in glove embeddings file\n",
    "city_to_changes = defaultdict(lambda: defaultdict(lambda: [0, 0])) # city to month to [trend, spike]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_words_file():\n",
    "    print 'Begin creating words file...'\n",
    "    with open(WORDS_FILE, 'w') as wf:\n",
    "        with open(PATH + EMBEDDINGS_FILE, 'r') as ef:\n",
    "            idx = 0\n",
    "            for line in ef:\n",
    "                word = line.split()[0]\n",
    "                wf.write(word + '\\n')\n",
    "                word_to_idx[word] = idx\n",
    "                idx += 1\n",
    "    print 'Finished creating words file!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin creating words file...\n",
      "Finished creating words file!\n"
     ]
    }
   ],
   "source": [
    "create_words_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_tweet_counts(month_to_tweets):\n",
    "    with open(TWEET_CNTS_FILE, 'w') as f:\n",
    "        f.write(','.join([str(len(month_to_tweets[month])) for month in month_to_tweets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_missing_words(missing_words):\n",
    "    with open(MISSING_WORDS_FILE, 'w') as f:\n",
    "        for word in missing_words:\n",
    "            f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_embeddings(month_to_tweets, city):\n",
    "    print 'Begin outputting word embeddings...'\n",
    "    for month in month_to_tweets:\n",
    "        if (len(month_to_tweets[month]) >= MIN_TWEETS):\n",
    "            with open('%s/embeddings/%s_%d_embeddings.csv' % (PATH[:-1], city, month), 'w') as output:\n",
    "                for tweet in month_to_tweets[month]:\n",
    "                    output.write(','.join([str(num) for num in tweet]) + '\\n')\n",
    "    print 'Finished outputting word embeddings!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_embeddings():\n",
    "    print 'Begin getting word embeddings...'\n",
    "    tweets_file = 'Delhi_tweets.csv'\n",
    "    title = 'delhi'\n",
    "    cnt = 0\n",
    "    with open(PATH + tweets_file) as csvfile:\n",
    "        month_to_tweets = defaultdict(list)\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        missing_words = set()\n",
    "        missing_words_cnt = 0\n",
    "        for row in reader:\n",
    "            if (cnt % 100000 == 0): print str(cnt) + ' tweets processed...'\n",
    "            cnt += 1\n",
    "            date = dateutil.parser.parse(row['postedTime'])\n",
    "            month_idx = (date.year - 2014)*12 + (date.month - 1)\n",
    "            tweet = ' '.join([word for word in casual_tokenize(row['tweet']) \n",
    "                              if '@' not in word and 'http' not in word and '#' not in word])\n",
    "            tweet = tweet.lower()\n",
    "            tweet_embedding = []\n",
    "            for word in tweet.split():\n",
    "                if (word in word_to_idx):\n",
    "                    tweet_embedding.append(word_to_idx[word])\n",
    "                else:\n",
    "                    missing_words_cnt += 1\n",
    "                    missing_words.add(word)\n",
    "            if (len(tweet_embedding) > 0): month_to_tweets[month_idx].append(tweet_embedding)\n",
    "            output_embeddings(month_to_tweets, title)\n",
    "            output_tweet_counts(month_to_tweets)\n",
    "            output_missing_words(missing_words)\n",
    "    print 'Finished getting word embeddings!'\n",
    "    print 'Number of missing words: ' + str(missing_words_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_word_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5ab23519700b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_word_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_word_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "get_word_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_deviations():\n",
    "    print 'Getting deviations...'\n",
    "    with open(PATH + 'India_Onion_Prices_Vector.csv') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader, None)\n",
    "        for row in reader:\n",
    "            city = row[0]\n",
    "            trends = row[2:NUM_MONTHS + 2]\n",
    "            spikes = row[NUM_MONTHS + 3:2*NUM_MONTHS + 3]\n",
    "            for i in range(NUM_MONTHS):\n",
    "                trend = int(trends[i]) + 1 if (trends[i] != 'NA') else -1\n",
    "                spike = int(spikes[i]) if (spikes[i] != 'NA') else -1\n",
    "                city_to_changes[city][i] = [trend, spike]\n",
    "    print ' -done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_relevant(tweet, keyword_embedding):\n",
    "    for word in tweet.split():\n",
    "        if (word in word_to_embedding):\n",
    "            embedding = np.array(word_to_embedding[word])\n",
    "            dist = np.linalg.norm(embedding - keyword_embedding)\n",
    "            if (dist <= MIN_DIST):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_batches(month_to_tweets, city):\n",
    "    num_batches = 0\n",
    "    for month in month_to_tweets:\n",
    "        tweet_count = len(month_to_tweets[month])\n",
    "        trend = city_to_changes[city][month][0]\n",
    "        spike = city_to_changes[city][month][1]\n",
    "        if (tweet_count >= MIN_TWEETS and trend != -1 and spike != -1):\n",
    "            num_batches += create_batches(tweet_count, city, month, trend, spike)\n",
    "    return num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_batches(tweet_count, city_name, month_idx, trend, spike):\n",
    "    np.random.seed(10)\n",
    "    \n",
    "    n = tweet_count\n",
    "    rand_seq = []\n",
    "    for i in range(NUM_RESAMPLES):\n",
    "        cur = [str(num) for num in np.random.choice(n, n, replace=False)]\n",
    "        remaining = K - (n % K) # used to make the length of rand_seq a multiple of K\n",
    "        cur += [str(num) for num in np.random.choice(n, remaining, replace=False)]\n",
    "        rand_seq += cur\n",
    "\n",
    "    num_batches = len(rand_seq) / K\n",
    "    folder = 'batches2'\n",
    "    output_file = '%s%s/%s_%s_batch.txt' % (PATH, folder, city_name, str(month_idx))\n",
    "    with open(output_file, 'w') as output:\n",
    "        output.write('%d,%d\\n' % (trend, spike))\n",
    "        for i in range(num_batches):\n",
    "            suffix = '\\n'\n",
    "            if (i == num_batches - 1): suffix = ''\n",
    "            output.write('\\t'.join(rand_seq[i * K : (i + 1) * K]) + suffix)\n",
    "    \n",
    "    with open(PATH + 'batch_counts.txt', 'a') as f:\n",
    "        f.write('%s,%d,%d\\n' % (city_name, month_idx, num_batches))\n",
    "        \n",
    "    return num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_relevant_word_embeddings(keyword):\n",
    "    keyword_embedding = np.array(word_to_embedding[keyword])\n",
    "    total_batches = 0\n",
    "    \n",
    "    for tweets_file in os.listdir(PATH):\n",
    "        if ('.csv' not in tweets_file) or ('India_Onion_Prices' in tweets_file): continue\n",
    "        if ('Mumbai' not in tweets_file): continue\n",
    "        print 'Reading ' + tweets_file + '...'\n",
    "        city = tweets_file.split('_')[0]\n",
    "        tweets_processed = 0\n",
    "        month_to_tweets = defaultdict(list)\n",
    "\n",
    "        with open(PATH + tweets_file) as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                if (tweets_processed % 1e5 == 0): print str(tweets_processed) + ' tweets processed...'\n",
    "                tweets_processed += 1\n",
    "                date = dateutil.parser.parse(row['postedTime'])\n",
    "                month_idx = (date.year - 2014)*12 + (date.month - 1)\n",
    "                tweet = ' '.join([word for word in casual_tokenize(row['tweet']) \n",
    "                                  if '@' not in word and 'http' not in word and '#' not in word])\n",
    "                tweet = tweet.lower()\n",
    "                tweet_embedding = []\n",
    "                if (is_relevant(tweet, keyword_embedding)):\n",
    "                    for word in tweet.split():\n",
    "                        if (word in word_to_idx):\n",
    "                            tweet_embedding.append(word_to_idx[word])\n",
    "                if (len(tweet_embedding) > 0): \n",
    "                    month_to_tweets[month_idx].append(tweet_embedding)\n",
    "                \n",
    "        output_embeddings(month_to_tweets, city)\n",
    "        \n",
    "#         print 'Outputting batches...'\n",
    "#         batches_created = output_batches(month_to_tweets, city)\n",
    "        \n",
    "#         total_batches += batches_created\n",
    "#         print 'Batches so far: %d' % total_batches\n",
    "#         print ' -done.'\n",
    "    \n",
    "#     print 'Total batches: %d' % total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-beb38c312cad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reads embeddings files and creates word_to_embedding map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mEMBEDDINGS_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'read_file' is not defined"
     ]
    }
   ],
   "source": [
    "# Reads embeddings files and creates word_to_embedding map\n",
    "read_file(PATH + EMBEDDINGS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting deviations...\n",
      " -done.\n"
     ]
    }
   ],
   "source": [
    "# Stores all price changes and spikes in city_to_changes\n",
    "get_deviations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_relevant_word_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b2f37137ff86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creates all tweet embeddings related to given parameter and outputs batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_relevant_word_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'onion'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_relevant_word_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# Creates all tweet embeddings related to given parameter and outputs batches\n",
    "get_relevant_word_embeddings('onion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets: 638606\n",
      "city: Chandigarh, batches: 1328\n",
      "city: Gurgaon, batches: 4040\n",
      "city: Chennai, batches: 1992\n",
      "city: Lucknow, batches: 1376\n",
      "city: Hyderabad, batches: 4128\n",
      "city: Delhi, batches: 24560\n",
      "city: Mumbai, batches: 38520\n",
      "city: Bengaluru, batches: 18424\n",
      "city: Jaipur, batches: 1264\n",
      "city: Kolkata, batches: 6952\n",
      "Total number of batches: 102584\n"
     ]
    }
   ],
   "source": [
    "city_to_tweets = defaultdict(lambda: [0 for i in range(NUM_MONTHS)])\n",
    "\n",
    "# Read embeddings files\n",
    "def read_embeddings_files():\n",
    "    path = '../data/embeddings/'\n",
    "    total = 0\n",
    "    for embeddings_file in os.listdir(path):\n",
    "        filename = embeddings_file.split('_')\n",
    "        city = filename[0]\n",
    "        month = int(filename[1])\n",
    "        with open(path + embeddings_file, 'r') as csvfile:\n",
    "            num_tweets = sum(1 for row in csv.reader(csvfile))\n",
    "            city_to_tweets[city][month] += num_tweets\n",
    "            total += num_tweets\n",
    "    print 'Total number of tweets: %d' % total\n",
    "            \n",
    "# Outputs batches by reading embeddings files to get tweet counts\n",
    "def output_batches_1():\n",
    "    num_batches = 0\n",
    "    for city in city_to_tweets:\n",
    "        city_batches = 0\n",
    "        for month in range(NUM_MONTHS):\n",
    "            tweet_count = city_to_tweets[city][month]\n",
    "            trend = city_to_changes[city][month][0]\n",
    "            spike = city_to_changes[city][month][1]\n",
    "            if (tweet_count >= MIN_TWEETS and trend != -1 and spike != -1):\n",
    "                batches = create_batches(tweet_count, city, month, trend, spike)\n",
    "                city_batches += batches\n",
    "        \n",
    "        num_batches += city_batches\n",
    "        print 'city: %s, batches: %d' % (city, city_batches)\n",
    "        \n",
    "    print 'Total number of batches: %d' % num_batches\n",
    "\n",
    "read_embeddings_files()\n",
    "output_batches_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_weekly_batches(city_name, tweet_counts, tweet_idxes, trends, label):\n",
    "    np.random.seed(10)\n",
    "    \n",
    "    time_len = len(tweet_counts)\n",
    "    weekly_rand_seqs = []\n",
    "    \n",
    "    for week in range(time_len):\n",
    "        n = tweet_counts[week]\n",
    "        idxes = tweet_idxes[week]\n",
    "        rand_seq = []\n",
    "        for i in range(NUM_RESAMPLES):\n",
    "            cur = [str(num) for num in np.random.choice(idxes, n, replace=False)]\n",
    "            remaining = K - (n % K) # used to make the length of rand_seq a multiple of K\n",
    "            cur += [str(num) for num in np.random.choice(idxes, remaining, replace=False)]\n",
    "            rand_seq += cur\n",
    "        weekly_rand_seqs.append(rand_seq)\n",
    "    \n",
    "    num_batches = min([len(weekly_rand_seqs[i]) for i in range(time_len)]) / K\n",
    "    folder = 'batches'\n",
    "    output_file = '%s%s/%s_weekly_batch.txt' % (PATH, folder, city_name)\n",
    "    with open(output_file, 'a') as output:\n",
    "        for i in range(num_batches):\n",
    "            suffix = '\\n'\n",
    "            if (i == num_batches - 1): suffix = ''\n",
    "            batch = ''\n",
    "            for week in range(time_len):\n",
    "                batch += '\\t'.join(weekly_rand_seqs[week][i * K : (i + 1) * K]) + '\\t'\n",
    "            trendstr = '\\t'.join([str(x) for x in trends])\n",
    "            batch = batch + trendstr + '\\t' + str(label)\n",
    "            output.write(batch + suffix)\n",
    "            assert(len(batch.split('\\t')) == 154)\n",
    "        \n",
    "    return num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_embeddings_by_city(tweets, city):\n",
    "    print 'Begin outputting word embeddings...'\n",
    "    with open('%s/embeddings/%s_embeddings.csv' % (PATH[:-1], city), 'w') as output:\n",
    "        for tweet in tweets:\n",
    "            output.write(','.join([str(num) for num in tweet]) + '\\n')\n",
    "    print 'Finished outputting word embeddings!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns map from city to (valid date (of fourth week), trends (of previous three weeks), label (of current week))\n",
    "def get_valid_city_weeks():\n",
    "    city_to_weekly_prices = get_city_to_weeks()\n",
    "    city_to_valid_weeks = defaultdict(lambda: defaultdict(list))\n",
    "    for city in city_to_weekly_prices:  \n",
    "        all_weeks = city_to_weekly_prices[city]\n",
    "        for week in range(len(all_weeks)):\n",
    "            if (week < 4): continue\n",
    "            cur = all_weeks[week]\n",
    "            cur_date = date(cur[0], cur[1], cur[2])\n",
    "            is_valid = True\n",
    "            for i in range(3):\n",
    "                prev_week = all_weeks[week - i - 1]\n",
    "                if cur_date - date(prev_week[0], prev_week[1], prev_week[2]) > timedelta(days = (i + 1)*7):\n",
    "                    is_valid = False\n",
    "                    break\n",
    "            if (is_valid):\n",
    "                prev_trends = [all_weeks[i][3] for i in range(week - 3, week)]\n",
    "                city_to_valid_weeks[city][cur_date] = [prev_trends, cur[3]]\n",
    "    return city_to_valid_weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs all relevant word embeddings in a city to one city file: 'city_embeddings.csv'\n",
    "def get_relevant_word_embeddings_by_city(keyword):\n",
    "    city_to_valid_weeks = get_valid_city_weeks()\n",
    "    \n",
    "    keyword_embedding = np.array(word_to_embedding[keyword])\n",
    "    total_batches = 0     \n",
    "    \n",
    "    for tweets_file in os.listdir(TWEET_PATH):\n",
    "        if ('.csv' not in tweets_file) or ('India_Onion_Prices' in tweets_file): continue\n",
    "        print 'Reading ' + tweets_file + '...'\n",
    "        city = tweets_file.split('_')[0].lower()\n",
    "        if city not in city_to_valid_weeks:\n",
    "            continue\n",
    "        valid_weeks = city_to_valid_weeks[city]\n",
    "        tweet_idx = 0\n",
    "        tweets_processed = 0\n",
    "        city_tweets = []\n",
    "        date_to_valid_tweets = defaultdict(lambda: [[0, 0, 0], [[], [], []]]) # date to list of [tweet cnts, tweet idxes]\n",
    "        \n",
    "        with open(TWEET_PATH + tweets_file) as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                if (tweets_processed % 1e5 == 0): print str(tweets_processed) + ' tweets processed...'\n",
    "                tweets_processed += 1\n",
    "                tweet_date = date(int(row['postedTime'][0:4]), int(row['postedTime'][5:7]), int(row['postedTime'][8:10]))\n",
    "                    \n",
    "                tweet = ' '.join([word for word in casual_tokenize(row['tweet']) \n",
    "                                  if '@' not in word and 'http' not in word and '#' not in word])\n",
    "                tweet = tweet.lower()\n",
    "                tweet_embedding = []\n",
    "                if (is_relevant(tweet, keyword_embedding)):\n",
    "                    for word in tweet.split():\n",
    "                        if (word in word_to_idx):\n",
    "                            tweet_embedding.append(word_to_idx[word])\n",
    "                if (len(tweet_embedding) > 0): \n",
    "                    city_tweets.append(tweet_embedding) \n",
    "                    \n",
    "                    for valid_date in valid_weeks:\n",
    "                        prev_week = None\n",
    "                        if (valid_date - tweet_date <= timedelta(days = 7)):\n",
    "                            prev_week = 0\n",
    "                        elif(valid_date - tweet_date <= timedelta(days = 14)):\n",
    "                            prev_week = 1\n",
    "                        elif(valid_date - tweet_date <= timedelta(days = 21)):\n",
    "                            prev_week = 2\n",
    "\n",
    "                        if prev_week == None: continue\n",
    "                        date_to_valid_tweets[valid_date][0][prev_week] += 1\n",
    "                        date_to_valid_tweets[valid_date][1][prev_week].append(tweet_idx)  \n",
    "                    \n",
    "                    tweet_idx += 1\n",
    "                \n",
    "        output_embeddings_by_city(city_tweets, city)\n",
    "        \n",
    "        total_batches = 0\n",
    "        for valid_date in date_to_valid_tweets:\n",
    "            tweet_info = date_to_valid_tweets[valid_date]\n",
    "            tweet_counts = tweet_info[0]\n",
    "            tweet_idxes = tweet_info[1]\n",
    "            trends = valid_weeks[valid_date][0]\n",
    "            label = valid_weeks[valid_date][1]\n",
    "            \n",
    "            valid_batch = True\n",
    "            for cnt in tweet_counts:\n",
    "                if cnt < MIN_TWEETS:\n",
    "                    valid_batch = False\n",
    "                    break        \n",
    "            if (valid_batch):\n",
    "                total_batches += create_weekly_batches(city, tweet_counts, tweet_idxes, trends, label)\n",
    "        print city, total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file...\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      " -done\n"
     ]
    }
   ],
   "source": [
    "read_file(PATH + EMBEDDINGS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Kohima_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "kohima 0\n",
      "Reading Cuttack_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "cuttack 0\n",
      "Reading Lucknow_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "400000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "lucknow 9220\n",
      "Reading Panchkula_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "panchkula 50\n",
      "Reading Siliguri_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "siliguri 0\n",
      "Reading Delhi_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "400000 tweets processed...\n",
      "500000 tweets processed...\n",
      "600000 tweets processed...\n",
      "700000 tweets processed...\n",
      "800000 tweets processed...\n",
      "900000 tweets processed...\n",
      "1000000 tweets processed...\n",
      "1100000 tweets processed...\n",
      "1200000 tweets processed...\n",
      "1300000 tweets processed...\n",
      "1400000 tweets processed...\n",
      "1500000 tweets processed...\n",
      "1600000 tweets processed...\n",
      "1700000 tweets processed...\n",
      "1800000 tweets processed...\n",
      "1900000 tweets processed...\n",
      "2000000 tweets processed...\n",
      "2100000 tweets processed...\n",
      "2200000 tweets processed...\n",
      "2300000 tweets processed...\n",
      "2400000 tweets processed...\n",
      "2500000 tweets processed...\n",
      "2600000 tweets processed...\n",
      "2700000 tweets processed...\n",
      "2800000 tweets processed...\n",
      "2900000 tweets processed...\n",
      "3000000 tweets processed...\n",
      "3100000 tweets processed...\n",
      "3200000 tweets processed...\n",
      "3300000 tweets processed...\n",
      "3400000 tweets processed...\n",
      "3500000 tweets processed...\n",
      "3600000 tweets processed...\n",
      "3700000 tweets processed...\n",
      "3800000 tweets processed...\n",
      "3900000 tweets processed...\n",
      "4000000 tweets processed...\n",
      "4100000 tweets processed...\n",
      "4200000 tweets processed...\n",
      "4300000 tweets processed...\n",
      "4400000 tweets processed...\n",
      "4500000 tweets processed...\n",
      "4600000 tweets processed...\n",
      "4700000 tweets processed...\n",
      "4800000 tweets processed...\n",
      "4900000 tweets processed...\n",
      "5000000 tweets processed...\n",
      "5100000 tweets processed...\n",
      "5200000 tweets processed...\n",
      "5300000 tweets processed...\n",
      "5400000 tweets processed...\n",
      "5500000 tweets processed...\n",
      "5600000 tweets processed...\n",
      "5700000 tweets processed...\n",
      "5800000 tweets processed...\n",
      "5900000 tweets processed...\n",
      "6000000 tweets processed...\n",
      "6100000 tweets processed...\n",
      "6200000 tweets processed...\n",
      "6300000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "delhi 0\n",
      "Reading Ernakulam_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "ernakulam 0\n",
      "Reading Ahmedabad_tweets.csv...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "ahmedabad 0\n",
      "Reading Bathinda_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "bathinda 425\n",
      "Reading Sambalpur_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "sambalpur 0\n",
      "Reading Ranchi_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "ranchi 640\n",
      "Reading Bengaluru_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "400000 tweets processed...\n",
      "500000 tweets processed...\n",
      "600000 tweets processed...\n",
      "700000 tweets processed...\n",
      "800000 tweets processed...\n",
      "900000 tweets processed...\n",
      "1000000 tweets processed...\n",
      "1100000 tweets processed...\n",
      "1200000 tweets processed...\n",
      "1300000 tweets processed...\n",
      "1400000 tweets processed...\n",
      "1500000 tweets processed...\n",
      "1600000 tweets processed...\n",
      "1700000 tweets processed...\n",
      "1800000 tweets processed...\n",
      "1900000 tweets processed...\n",
      "2000000 tweets processed...\n",
      "2100000 tweets processed...\n",
      "2200000 tweets processed...\n",
      "2300000 tweets processed...\n",
      "2400000 tweets processed...\n",
      "2500000 tweets processed...\n",
      "2600000 tweets processed...\n",
      "2700000 tweets processed...\n",
      "2800000 tweets processed...\n",
      "2900000 tweets processed...\n",
      "3000000 tweets processed...\n",
      "3100000 tweets processed...\n",
      "3200000 tweets processed...\n",
      "3300000 tweets processed...\n",
      "3400000 tweets processed...\n",
      "3500000 tweets processed...\n",
      "3600000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "bengaluru 0\n",
      "Reading Shillong_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "shillong 0\n",
      "Reading Kanpur_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "kanpur 2825\n",
      "Reading Mumbai_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "400000 tweets processed...\n",
      "500000 tweets processed...\n",
      "600000 tweets processed...\n",
      "700000 tweets processed...\n",
      "800000 tweets processed...\n",
      "900000 tweets processed...\n",
      "1000000 tweets processed...\n",
      "1100000 tweets processed...\n",
      "1200000 tweets processed...\n",
      "1300000 tweets processed...\n",
      "1400000 tweets processed...\n",
      "1500000 tweets processed...\n",
      "1600000 tweets processed...\n",
      "1700000 tweets processed...\n",
      "1800000 tweets processed...\n",
      "1900000 tweets processed...\n",
      "2000000 tweets processed...\n",
      "2100000 tweets processed...\n",
      "2200000 tweets processed...\n",
      "2300000 tweets processed...\n",
      "2400000 tweets processed...\n",
      "2500000 tweets processed...\n",
      "2600000 tweets processed...\n",
      "2700000 tweets processed...\n",
      "2800000 tweets processed...\n",
      "2900000 tweets processed...\n",
      "3000000 tweets processed...\n",
      "3100000 tweets processed...\n",
      "3200000 tweets processed...\n",
      "3300000 tweets processed...\n",
      "3400000 tweets processed...\n",
      "3500000 tweets processed...\n",
      "3600000 tweets processed...\n",
      "3700000 tweets processed...\n",
      "3800000 tweets processed...\n",
      "3900000 tweets processed...\n",
      "4000000 tweets processed...\n",
      "4100000 tweets processed...\n",
      "4200000 tweets processed...\n",
      "4300000 tweets processed...\n",
      "4400000 tweets processed...\n",
      "4500000 tweets processed...\n",
      "4600000 tweets processed...\n",
      "4700000 tweets processed...\n",
      "4800000 tweets processed...\n",
      "4900000 tweets processed...\n",
      "5000000 tweets processed...\n",
      "5100000 tweets processed...\n",
      "5200000 tweets processed...\n",
      "5300000 tweets processed...\n",
      "5400000 tweets processed...\n",
      "5500000 tweets processed...\n",
      "5600000 tweets processed...\n",
      "5700000 tweets processed...\n",
      "5800000 tweets processed...\n",
      "5900000 tweets processed...\n",
      "6000000 tweets processed...\n",
      "6100000 tweets processed...\n",
      "6200000 tweets processed...\n",
      "6300000 tweets processed...\n",
      "6400000 tweets processed...\n",
      "6500000 tweets processed...\n",
      "6600000 tweets processed...\n",
      "6700000 tweets processed...\n",
      "6800000 tweets processed...\n",
      "6900000 tweets processed...\n",
      "7000000 tweets processed...\n",
      "7100000 tweets processed...\n",
      "7200000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "mumbai 222185\n",
      "Reading Hyderabad_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "400000 tweets processed...\n",
      "500000 tweets processed...\n",
      "600000 tweets processed...\n",
      "700000 tweets processed...\n",
      "800000 tweets processed...\n",
      "900000 tweets processed...\n",
      "1000000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "hyderabad 27765\n",
      "Reading Chennai_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "chennai 0\n",
      "Reading Thiruchirapalli_tweets.csv...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "thiruchirapalli 0\n",
      "Reading Jabalpur_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "jabalpur 0\n",
      "Reading Shimla_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "shimla 0\n",
      "Reading Agra_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "agra 375\n",
      "Reading Vijaywada_tweets.csv...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "vijaywada 0\n",
      "Reading Bhopal_tweets.csv...\n",
      "0 tweets processed...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "bhopal 3945\n",
      "Reading Jammu_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "jammu 5495\n",
      "Reading Jaipur_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "400000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "jaipur 5630\n",
      "Reading Srinagar_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "srinagar 0\n",
      "Reading Rourkela_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "rourkela 0\n",
      "Reading Puducherry_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "puducherry 0\n",
      "Reading Ludhiana_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "ludhiana 3180\n",
      "Reading Nagpur_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "nagpur 5250\n",
      "Reading Karnal_tweets.csv...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "karnal 0\n",
      "Reading Dehradun_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "dehradun 0\n",
      "Reading Patna_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "patna 3870\n",
      "Reading Dindigul_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "dindigul 0\n",
      "Reading Rajkot_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "rajkot 280\n",
      "Reading Bhagalpur_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "bhagalpur 0\n",
      "Reading Guwahati_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "guwahati 4065\n",
      "Reading Raipur_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "raipur 310\n",
      "Reading Port Blair_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "port blair 0\n",
      "Reading Agartala_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "agartala 0\n",
      "Reading Trivandrum_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "trivandrum 0\n",
      "Reading Jodhpur_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "jodhpur 900\n",
      "Reading Gwalior_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "gwalior 0\n",
      "Reading Amritsar_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "amritsar 585\n",
      "Reading Itanagar_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "itanagar 0\n",
      "Reading Bhubaneshwar_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "bhubaneshwar 0\n",
      "Reading Kolkata_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "400000 tweets processed...\n",
      "500000 tweets processed...\n",
      "600000 tweets processed...\n",
      "700000 tweets processed...\n",
      "800000 tweets processed...\n",
      "900000 tweets processed...\n",
      "1000000 tweets processed...\n",
      "1100000 tweets processed...\n",
      "1200000 tweets processed...\n",
      "1300000 tweets processed...\n",
      "1400000 tweets processed...\n",
      "1500000 tweets processed...\n",
      "1600000 tweets processed...\n",
      "1700000 tweets processed...\n",
      "1800000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "kolkata 0\n",
      "Reading Panaji_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "panaji 50\n",
      "Reading Gurgaon_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "400000 tweets processed...\n",
      "500000 tweets processed...\n",
      "600000 tweets processed...\n",
      "700000 tweets processed...\n",
      "800000 tweets processed...\n",
      "900000 tweets processed...\n",
      "1000000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "gurgaon 29775\n",
      "Reading Hisar_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "hisar 0\n",
      "Reading Mandi_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "mandi 0\n",
      "Reading Aizwal_tweets.csv...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "aizwal 0\n",
      "Reading Kozhidoke_tweets.csv...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "kozhidoke 0\n",
      "Reading Indore_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "indore 3685\n",
      "Reading Kota_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "kota 0\n",
      "Reading Chandigarh_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "200000 tweets processed...\n",
      "300000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "chandigarh 8230\n",
      "Reading Dharwad_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "dharwad 0\n",
      "Reading Varanasi_tweets.csv...\n",
      "0 tweets processed...\n",
      "100000 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "varanasi 2755\n",
      "Reading Dimapur_tweets.csv...\n",
      "0 tweets processed...\n",
      "Begin outputting word embeddings...\n",
      "Finished outputting word embeddings!\n",
      "dimapur 0\n"
     ]
    }
   ],
   "source": [
    "get_relevant_word_embeddings_by_city('onion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
